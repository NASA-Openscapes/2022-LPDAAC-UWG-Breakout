[
  {
    "objectID": "cloud-paradigm.html",
    "href": "cloud-paradigm.html",
    "title": "NASA and the Cloud Paradigm",
    "section": "",
    "text": "Slides that introduce NASA Earthdata Cloud & the Cloud Paradigm."
  },
  {
    "objectID": "notebooks/Getting_Started/Intro_xarray_hvplot.html",
    "href": "notebooks/Getting_Started/Intro_xarray_hvplot.html",
    "title": "2022 LP DAAC UWG Cloud Workshop",
    "section": "",
    "text": "As Geoscientists, we often work with time series of data with two or more dimensions: a time series of calibrated, orthorectified satellite images; two-dimensional grids of surface air temperature from an atmospheric reanalysis; or three-dimensional (level, x, y) cubes of ocean salinity from an ocean model. These data are often provided in GeoTIFF, NetCDF or HDF format with rich and useful metadata that we want to retain, or even use in our analysis. Common analyses include calculating means, standard deviations and anomalies over time or one or more spatial dimensions (e.g. zonal means). Model output often includes multiple variables that you want to apply similar analyses to.\n\n\n\nA schematic of multi-dimensional data\n\n\nThe schematic above shows a typical data structure for multi-dimensional data. There are two data cubes, one for temperature and one for precipitation. Common coordinate variables, in this case latitude, longitude and time are associated with each variable. Each variable, including coordinate variables, will have a set of attributes: name, units, missing value, etc. The file containing the data may also have attributes: source of the data, model name coordinate reference system if the data are projected. Writing code using low-level packages such as netcdf4 and numpy to read the data, then perform analysis, and write the results to file is time consuming and prone to errors.\n\n\n\nxarray is an open-source project and python package to work with labelled multi-dimensional arrays. It is leverages numpy, pandas, matplotlib and dask to build Dataset and DataArray objects with built-in methods to subset, analyze, interpolate, and plot multi-dimensional data. It makes working with multi-dimensional data cubes efficient and fun. It will change your life for the better. You’ll be more attractive, more interesting, and better equiped to take on lifes challenges.\n\n\n\nIn this tutorial you will learn how to:\n\nload a netcdf file into xarray\ninterrogate the Dataset and understand the difference between DataArray and Dataset\nsubset a Dataset\ncalculate annual and monthly mean fields\ncalculate a time series of zonal means\nplot these results\n\nAs always, we’ll start by importing xarray. We’ll follow convention by giving the module the shortname xr\n\nimport xarray as xr\nxr.set_options(keep_attrs=True)\nimport hvplot.xarray\n\nI’m going to use one of xarray’s tutorial datasets. In this case, air temperature from the NCEP reanalysis. I’ll assign the result of the open_dataset to ds. I may change this to access a dataset directly\n\nds = xr.tutorial.open_dataset(\"air_temperature\")\n\nAs we are in an interactive environment, we can just type ds to see what we have.\n\nds\n\nFirst thing to notice is that ds is an xarray.Dataset object. It has dimensions, lat, lon, and time. It also has coordinate variables with the same names as these dimensions. These coordinate variables are 1-dimensional. This is a NetCDF convention. The Dataset contains one data variable, air. This has dimensions (time, lat, lon).\nClicking on the document icon reveals attributes for each variable. Clicking on the disk icon reveals a representation of the data.\nEach of the data and coordinate variables can be accessed and examined using the variable name as a key.\n\nds.air\n\n\nds['air']\n\nThese are xarray.DataArray objects. This is the basic building block for xarray.\nVariables can also be accessed as attributes of ds.\n\nds.time\n\nA major difference between accessing a variable as an attribute versus using a key is that the attribute is read-only but the key method can be used to update the variable. For example, if I want to convert the units of air from Kelvin to degrees Celsius.\n\nds['air'] = ds.air - 273.15\n\nThis approach can also be used to add new variables\n\nds['air_kelvin'] = ds.air + 273.15\n\n\nds\n\nIt is helpful to update attributes such as units, this saves time, confusion and mistakes, especially when you save the dataset.\n\nds['air'].attrs['units'] = 'degC'\n\n\nds\n\n\n\n\nSubsetting and indexing methods depend on whether you are working with a Dataset or DataArray. A DataArray can be accessed using positional indexing just like a numpy array. To access the temperature field for the first time step, you do the following.\n\nds['air'][0,:,:]\n\nNote this returns a DataArray with coordinates but not attributes.\nHowever, the real power is being able to access variables using coordinate variables. I can get the same subset using the following. (It’s also more explicit about what is being selected and robust in case I modify the DataArray and expect the same output.)\n\nds['air'].sel(time='2013-01-01').time\n\n\nds.air.sel(time='2013-01-01')\n\nI can also do slices. I’ll extract temperatures for the state of Colorado. The bounding box for the state is [-109 E, -102 E, 37 N, 41 N].\nIn the code below, pay attention to both the order of the coordinates and the range of values. The first value of the lat coordinate variable is 41 N, the second value is 37 N. Unfortunately, xarray expects slices of coordinates to be in the same order as the coordinates. Note lon is 0 to 360 not -180 to 180, and I let python calculate it for me within the slice.\n\nds.air.sel(lat=slice(41.,37.), lon=slice(360-109,360-102))\n\nWhat if we want temperature for a point, for example Denver, CO (39.72510678889283 N, -104.98785545855408 E). xarray can handle this! If we just want data from the nearest grid point, we can use sel and specify the method as “nearest”.\n\ndenver_lat, denver_lon = 39.72510678889283, -104.98785545855408\n\n\nds.air.sel(lat=denver_lat, lon=360+denver_lon, method='nearest').hvplot()\n\nIf we want to interpolate, we can use interp(). In this case I use linear or bilinear interpolation.\ninterp() can also be used to resample data to a new grid and even reproject data\n\nds.air.interp(lat=denver_lat, lon=360+denver_lon, method='linear')\n\nsel() and interp() can also be used on Dataset objects.\n\nds.sel(lat=slice(41,37), lon=slice(360-109,360-102))\n\n\nds.interp(lat=denver_lat, lon=360+denver_lon, method='linear')\n\n\n\n\nAs a simple example, let’s try to calculate a mean field for the whole time range.\n\nds.mean(dim='time').hvplot()\n\nWe can also calculate a zonal mean (averaging over longitude)\n\nds.mean(dim='lon').hvplot()\n\nOther aggregation methods include min(), max(), std(), along with others.\n\nds.std(dim='time').hvplot()\n\nThe data we have are in 6h timesteps. This can be resampled to daily or monthly. If you are familiar with pandas, xarray uses the same methods.\n\nds_mon = ds.resample(time='M').mean()\nds_mon\n\nThis is a really short time series but as an example, let’s calculate a monthly climatology (at least for 2 months). For this we can use groupby()\n\nds_clim = ds_mon.groupby(ds_mon.time.dt.month).mean()\nds_clim\n\n\n\n\nFinally, let’s plot the results! This will plot the lat/lon axes of the original ds DataArray.\n\nds_clim.air.sel(month=10).hvplot()"
  },
  {
    "objectID": "notebooks/Getting_Started/NASA_Earthdata_Authentication.html",
    "href": "notebooks/Getting_Started/NASA_Earthdata_Authentication.html",
    "title": "2022 LP DAAC UWG Cloud Workshop",
    "section": "",
    "text": "This notebook creates a hidden .netrc file (_netrc for Window OS) with Earthdata login credentials in your home directory. This file is needed to access NASA Earthdata assets from a scripting environment like Python.\n\n\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. An example of the required content is below.\nmachine urs.earthdata.nasa.gov\nlogin <USERNAME>\npassword <PASSWORD>\n<USERNAME> and <PASSWORD> would be replaced by your actual Earthdata Login username and password respectively.\n\n\n\n\n\nfrom netrc import netrc\nfrom subprocess import Popen\nfrom platform import system\nfrom getpass import getpass\nimport os\n\nThe code below will:\n\ncheck what operating system (OS) is being used to determine which netrc file to check for/create (.netrc or _netrc)\ncheck if you have an netrc file, and if so, varify if those credentials are for the Earthdata endpoint\ncreate a netrc file if a netrc file is not present.\n\n\nurs = 'urs.earthdata.nasa.gov'    # Earthdata URL endpoint for authentication\nprompts = ['Enter NASA Earthdata Login Username: ',\n           'Enter NASA Earthdata Login Password: ']\n\n# Determine the OS (Windows machines usually use an '_netrc' file)\nnetrc_name = \"_netrc\" if system()==\"Windows\" else \".netrc\"\n\n# Determine if netrc file exists, and if so, if it includes NASA Earthdata Login Credentials\ntry:\n    netrcDir = os.path.expanduser(f\"~/{netrc_name}\")\n    netrc(netrcDir).authenticators(urs)[0]\n\n# Below, create a netrc file and prompt user for NASA Earthdata Login Username and Password\nexcept FileNotFoundError:\n    homeDir = os.path.expanduser(\"~\")\n    Popen('touch {0}{2} | echo machine {1} >> {0}{2}'.format(homeDir + os.sep, urs, netrc_name), shell=True)\n    Popen('echo login {} >> {}{}'.format(getpass(prompt=prompts[0]), homeDir + os.sep, netrc_name), shell=True)\n    Popen('echo \\'password {} \\'>> {}{}'.format(getpass(prompt=prompts[1]), homeDir + os.sep, netrc_name), shell=True)\n    # Set restrictive permissions\n    Popen('chmod 0600 {0}{1}'.format(homeDir + os.sep, netrc_name), shell=True)\n\n    # Determine OS and edit netrc file if it exists but is not set up for NASA Earthdata Login\nexcept TypeError:\n    homeDir = os.path.expanduser(\"~\")\n    Popen('echo machine {1} >> {0}{2}'.format(homeDir + os.sep, urs, netrc_name), shell=True)\n    Popen('echo login {} >> {}{}'.format(getpass(prompt=prompts[0]), homeDir + os.sep, netrc_name), shell=True)\n    Popen('echo \\'password {} \\'>> {}{}'.format(getpass(prompt=prompts[1]), homeDir + os.sep, netrc_name), shell=True)\n\n\n\nIf the file was created, we’ll see a .netrc file (_netrc for Window OS) in the list printed below. To view the contents from a Jupyter environment, click File on the top toolbar, select Open from Path…, type .netrc, and click Open. The .netrc file will open within the text editor.\n\n!!! Beware, your password will be visible if the .netrc file is opened in the text editor.\n\n\n!ls -al ~/"
  },
  {
    "objectID": "notebooks/Data_Access_and_Processing/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html",
    "href": "notebooks/Data_Access_and_Processing/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html",
    "title": "2022 LP DAAC UWG Cloud Workshop",
    "section": "",
    "text": "In this notebook, we will access data for the Harmonized Landsat Sentinel-2 (HLS) Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0 (L30) (10.5067/HLS/HLSL30.002) data product. These data are archived and distributed as Cloud Optimized GeoTIFF (COG) files, one file for each spectral band.\nWe will access a single COG file, L30 red band (0.64 – 0.67 μm), from inside the AWS cloud (us-west-2 region, specifically) and load it into Python as an xarray dataarray. This approach leverages S3 native protocols for efficient access to the data.\n\n\n\n\n\nNASA Earthdata Cloud data in S3 can be directly accessed via temporary credentials; this access is limited to requests made within the US West (Oregon) (code: us-west-2) AWS region.\n\n\n\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. For additional information see: Authentication for NASA Earthdata.\n\n\n\n\n\nhow to configure you Python work environment to access Cloud Optimized geoTIFF (COG) files\nhow to access HLS COG files\nhow to plot the data\n\n\n\n\n\nUsing Harmonized Landsat Sentinel-2 (HLS) version 2.0\n\n\n\nimport os\nfrom osgeo import gdal\nimport rasterio as rio\nimport rioxarray\nimport hvplot.xarray\nimport holoviews as hv\n\n\n\n\n\nFor this exercise, we are going to open up a context manager for the notebook using the rasterio.env module to store the required GDAL configurations we need to access the data from Earthdata Cloud. While the context manager is open (rio_env.__enter__()) we will be able to run the open or get data commands that would typically be executed within a with statement, thus allowing us to more freely interact with the data. We’ll close the context (rio_env.__exit__()) at the end of the notebook.\nGDAL environment variables must be configured to access COGs from Earthdata Cloud. Geospatial data access Python packages like rasterio and rioxarray depend on GDAL, leveraging GDAL’s “Virtual File Systems” to read remote files. GDAL has a lot of environment variables that control it’s behavior. Changing these settings can mean the difference being able to access a file or not. They can also have an impact on the performance.\n\nrio_env = rio.Env(GDAL_DISABLE_READDIR_ON_OPEN='TRUE',\n                  GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n                  GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'))\nrio_env.__enter__()\n\nIn this example we’re interested in the HLS L30 data collection from NASA’s LP DAAC in Earthdata Cloud. Below we specify the s3 URL to the data asset in Earthdata Cloud. This URL can be found via Earthdata Search or programmatically through the CMR and CMR-STAC APIs.\n\nhttps_url = 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T11SQA.2021333T181532.v2.0/HLS.L30.T11SQA.2021333T181532.v2.0.B04.tif'\n\n\n\n\nRead in the HLS s3 URL for the L30 red band (0.64 – 0.67 μm) into our workspace using rioxarray, an extension of xarray used to read geospatial data.\n\nda = rioxarray.open_rasterio(https_url)\nda\n\nThe file is read into Python as an xarray dataarray with a band, x, and y dimension. In this example the band dimension is meaningless, so we’ll use the squeeze() function to remove band as a dimension.\n\nda_red = da.squeeze('band', drop=True)\nda_red\n\nPlot the dataarray, representing the L30 red band, using hvplot.\n\nda_red.hvplot.image(x='x', y='y', cmap='gray', aspect='equal')\n\nExit the context manager.\n\nrio_env.__exit__()"
  },
  {
    "objectID": "notebooks/Data_Access_and_Processing/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html",
    "href": "notebooks/Data_Access_and_Processing/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html",
    "title": "2022 LP DAAC UWG Cloud Workshop",
    "section": "",
    "text": "In this notebook, we will access monthly sea surface height from ECCO V4r4 (10.5067/ECG5D-SSH44). The data are provided as a time series of monthly netCDFs on a 0.5-degree latitude/longitude grid.\nWe will access a single netCDF file from inside the AWS cloud (us-west-2 region, specifically) and load it into Python as an xarray dataset. This approach leverages S3 native protocols for efficient access to the data.\n\n\n\n\n\nNASA Earthdata Cloud data in S3 can be directly accessed via temporary credentials; this access is limited to requests made within the US West (Oregon) (code: us-west-2) AWS region.\n\n\n\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. For additional information see: Authentication for NASA Earthdata.\n\n\n\n\n\nhow to retrieve temporary S3 credentials for in-region direct S3 bucket access\nhow to perform in-region direct access of ECCO_L4_SSH_05DEG_MONTHLY_V4R4 data in S3\nhow to plot the data\n\n\n\n\n\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport os\nimport requests\nimport s3fs\nfrom osgeo import gdal\nimport xarray as xr\nimport hvplot.xarray\nimport holoviews as hv\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDirect S3 access is achieved by passing NASA supplied temporary credentials to AWS so we can interact with S3 objects from applicable Earthdata Cloud buckets. For now, each NASA DAAC has different AWS credentials endpoints. Below are some of the credential endpoints to various DAACs:\n\ns3_cred_endpoint = {\n    'podaac':'https://archive.podaac.earthdata.nasa.gov/s3credentials',\n    'gesdisc': 'https://data.gesdisc.earthdata.nasa.gov/s3credentials',\n    'lpdaac':'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials',\n    'ornldaac': 'https://data.ornldaac.earthdata.nasa.gov/s3credentials',\n    'ghrcdaac': 'https://data.ghrc.earthdata.nasa.gov/s3credentials'\n}\n\nCreate a function to make a request to an endpoint for temporary credentials. Remember, each DAAC has their own endpoint and credentials are not usable for cloud data from other DAACs.\n\ndef get_temp_creds(provider):\n    return requests.get(s3_cred_endpoint[provider]).json()\n\n\ntemp_creds_req = get_temp_creds('podaac')\n#temp_creds_req\n\n\n\n\ns3fs sessions are used for authenticated access to s3 bucket and allows for typical file-system style operations. Below we create session by passing in the temporary credentials we recieved from our temporary credentials endpoint.\n\nfs_s3 = s3fs.S3FileSystem(anon=False, \n                          key=temp_creds_req['accessKeyId'], \n                          secret=temp_creds_req['secretAccessKey'], \n                          token=temp_creds_req['sessionToken'])\n\nIn this example we’re interested in the ECCO data collection from NASA’s PO.DAAC in Earthdata Cloud. Below we specify the s3 URL to the data asset in Earthdata Cloud. This URL can be found via Earthdata Search or programmatically through the CMR and CMR-STAC APIs.\n\ns3_url = 's3://podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-01_ECCO_V4r4_latlon_0p50deg.nc'\n\n\n\n\nOpen with the netCDF file using the s3fs package, then load the cloud asset into an xarray dataset.\n\ns3_file_obj = fs_s3.open(s3_url, mode='rb')\n\n\nssh_ds = xr.open_dataset(s3_file_obj, engine='h5netcdf')\nssh_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:         (time: 1, latitude: 360, longitude: 720, nv: 2)\nCoordinates:\n  * time            (time) datetime64[ns] 2015-01-16T12:00:00\n  * latitude        (latitude) float32 -89.75 -89.25 -88.75 ... 89.25 89.75\n  * longitude       (longitude) float32 -179.8 -179.2 -178.8 ... 179.2 179.8\n    time_bnds       (time, nv) datetime64[ns] 2015-01-01 2015-02-01\n    latitude_bnds   (latitude, nv) float32 -90.0 -89.5 -89.5 ... 89.5 89.5 90.0\n    longitude_bnds  (longitude, nv) float32 -180.0 -179.5 -179.5 ... 179.5 180.0\nDimensions without coordinates: nv\nData variables:\n    SSH             (time, latitude, longitude) float32 ...\n    SSHIBC          (time, latitude, longitude) float32 ...\n    SSHNOIBC        (time, latitude, longitude) float32 ...\nAttributes: (12/57)\n    acknowledgement:              This research was carried out by the Jet Pr...\n    author:                       Ian Fenty and Ou Wang\n    cdm_data_type:                Grid\n    comment:                      Fields provided on a regular lat-lon grid. ...\n    Conventions:                  CF-1.8, ACDD-1.3\n    coordinates_comment:          Note: the global 'coordinates' attribute de...\n    ...                           ...\n    time_coverage_duration:       P1M\n    time_coverage_end:            2015-02-01T00:00:00\n    time_coverage_resolution:     P1M\n    time_coverage_start:          2015-01-01T00:00:00\n    title:                        ECCO Sea Surface Height - Monthly Mean 0.5 ...\n    uuid:                         088d03b8-4158-11eb-876b-0cc47a3f47f1xarray.DatasetDimensions:time: 1latitude: 360longitude: 720nv: 2Coordinates: (6)time(time)datetime64[ns]2015-01-16T12:00:00axis :Tbounds :time_bndscoverage_content_type :coordinatelong_name :center time of averaging periodstandard_name :timearray(['2015-01-16T12:00:00.000000000'], dtype='datetime64[ns]')latitude(latitude)float32-89.75 -89.25 ... 89.25 89.75axis :Ybounds :latitude_bndscomment :uniform grid spacing from -89.75 to 89.75 by 0.5coverage_content_type :coordinatelong_name :latitude at grid cell centerstandard_name :latitudeunits :degrees_northarray([-89.75, -89.25, -88.75, ...,  88.75,  89.25,  89.75], dtype=float32)longitude(longitude)float32-179.8 -179.2 ... 179.2 179.8axis :Xbounds :longitude_bndscomment :uniform grid spacing from -179.75 to 179.75 by 0.5coverage_content_type :coordinatelong_name :longitude at grid cell centerstandard_name :longitudeunits :degrees_eastarray([-179.75, -179.25, -178.75, ...,  178.75,  179.25,  179.75],\n      dtype=float32)time_bnds(time, nv)datetime64[ns]...comment :Start and end times of averaging period.coverage_content_type :coordinatelong_name :time bounds of averaging periodarray([['2015-01-01T00:00:00.000000000', '2015-02-01T00:00:00.000000000']],\n      dtype='datetime64[ns]')latitude_bnds(latitude, nv)float32...coverage_content_type :coordinatelong_name :latitude bounds grid cellsarray([[-90. , -89.5],\n       [-89.5, -89. ],\n       [-89. , -88.5],\n       ...,\n       [ 88.5,  89. ],\n       [ 89. ,  89.5],\n       [ 89.5,  90. ]], dtype=float32)longitude_bnds(longitude, nv)float32...coverage_content_type :coordinatelong_name :longitude bounds grid cellsarray([[-180. , -179.5],\n       [-179.5, -179. ],\n       [-179. , -178.5],\n       ...,\n       [ 178.5,  179. ],\n       [ 179. ,  179.5],\n       [ 179.5,  180. ]], dtype=float32)Data variables: (3)SSH(time, latitude, longitude)float32...coverage_content_type :modelResultlong_name :Dynamic sea surface height anomalystandard_name :sea_surface_height_above_geoidunits :mcomment :Dynamic sea surface height anomaly above the geoid, suitable for comparisons with altimetry sea surface height data products that apply the inverse barometer (IB) correction. Note: SSH is calculated by correcting model sea level anomaly ETAN for three effects: a) global mean steric sea level changes related to density changes in the Boussinesq volume-conserving model (Greatbatch correction, see sterGloH), b) the inverted barometer (IB) effect (see SSHIBC) and c) sea level displacement due to sea-ice and snow pressure loading (see sIceLoad). SSH can be compared with the similarly-named SSH variable in previous ECCO products that did not include atmospheric pressure loading (e.g., Version 4 Release 3). Use SSHNOIBC for comparisons with altimetry data products that do NOT apply the IB correction.valid_min :-1.8805772066116333valid_max :1.4207719564437866[259200 values with dtype=float32]SSHIBC(time, latitude, longitude)float32...coverage_content_type :modelResultlong_name :The inverted barometer (IB) correction to sea surface height due to atmospheric pressure loadingunits :mcomment :Not an SSH itself, but a correction to model sea level anomaly (ETAN) required to account for the static part of sea surface displacement by atmosphere pressure loading: SSH = SSHNOIBC - SSHIBC. Note: Use SSH for model-data comparisons with altimetry data products that DO apply the IB correction and SSHNOIBC for comparisons with altimetry data products that do NOT apply the IB correction.valid_min :-0.30144819617271423valid_max :0.5245633721351624[259200 values with dtype=float32]SSHNOIBC(time, latitude, longitude)float32...coverage_content_type :modelResultlong_name :Sea surface height anomaly without the inverted barometer (IB) correctionunits :mcomment :Sea surface height anomaly above the geoid without the inverse barometer (IB) correction, suitable for comparisons with altimetry sea surface height data products that do NOT apply the inverse barometer (IB) correction. Note: SSHNOIBC is calculated by correcting model sea level anomaly ETAN for two effects: a) global mean steric sea level changes related to density changes in the Boussinesq volume-conserving model (Greatbatch correction, see sterGloH), b) sea level displacement due to sea-ice and snow pressure loading (see sIceLoad). In ECCO Version 4 Release 4 the model is forced with atmospheric pressure loading. SSHNOIBC does not correct for the static part of the effect of atmosphere pressure loading on sea surface height (the so-called inverse barometer (IB) correction). Use SSH for comparisons with altimetry data products that DO apply the IB correction.valid_min :-1.6654272079467773valid_max :1.4550364017486572[259200 values with dtype=float32]Attributes: (57)acknowledgement :This research was carried out by the Jet Propulsion Laboratory, managed by the California Institute of Technology under a contract with the National Aeronautics and Space Administration.author :Ian Fenty and Ou Wangcdm_data_type :Gridcomment :Fields provided on a regular lat-lon grid. They have been mapped to the regular lat-lon grid from the original ECCO lat-lon-cap 90 (llc90) native model grid. SSH (dynamic sea surface height) = SSHNOIBC (dynamic sea surface without the inverse barometer correction) - SSHIBC (inverse barometer correction). The inverted barometer correction accounts for variations in sea surface height due to atmospheric pressure variations.Conventions :CF-1.8, ACDD-1.3coordinates_comment :Note: the global 'coordinates' attribute describes auxillary coordinates.creator_email :ecco-group@mit.educreator_institution :NASA Jet Propulsion Laboratory (JPL)creator_name :ECCO Consortiumcreator_type :groupcreator_url :https://ecco-group.orgdate_created :2020-12-18T09:39:51date_issued :2020-12-18T09:39:51date_metadata_modified :2021-03-15T22:07:49date_modified :2021-03-15T22:07:49geospatial_bounds_crs :EPSG:4326geospatial_lat_max :90.0geospatial_lat_min :-90.0geospatial_lat_resolution :0.5geospatial_lat_units :degrees_northgeospatial_lon_max :180.0geospatial_lon_min :-180.0geospatial_lon_resolution :0.5geospatial_lon_units :degrees_easthistory :Inaugural release of an ECCO Central Estimate solution to PO.DAACid :10.5067/ECG5M-SSH44institution :NASA Jet Propulsion Laboratory (JPL)instrument_vocabulary :GCMD instrument keywordskeywords :EARTH SCIENCE > OCEANS > SEA SURFACE TOPOGRAPHY > SEA SURFACE HEIGHT, EARTH SCIENCE SERVICES > MODELS > EARTH SCIENCE REANALYSES/ASSIMILATION MODELSkeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordslicense :Public Domainmetadata_link :https://cmr.earthdata.nasa.gov/search/collections.umm_json?ShortName=ECCO_L4_SSH_05DEG_MONTHLY_V4R4naming_authority :gov.nasa.jplplatform :ERS-1/2, TOPEX/Poseidon, Geosat Follow-On (GFO), ENVISAT, Jason-1, Jason-2, CryoSat-2, SARAL/AltiKa, Jason-3, AVHRR, Aquarius, SSM/I, SSMIS, GRACE, DTU17MDT, Argo, WOCE, GO-SHIP, MEOP, Ice Tethered Profilers (ITP)platform_vocabulary :GCMD platform keywordsprocessing_level :L4product_name :SEA_SURFACE_HEIGHT_mon_mean_2015-01_ECCO_V4r4_latlon_0p50deg.ncproduct_time_coverage_end :2018-01-01T00:00:00product_time_coverage_start :1992-01-01T12:00:00product_version :Version 4, Release 4program :NASA Physical Oceanography, Cryosphere, Modeling, Analysis, and Prediction (MAP)project :Estimating the Circulation and Climate of the Ocean (ECCO)publisher_email :podaac@podaac.jpl.nasa.govpublisher_institution :PO.DAACpublisher_name :Physical Oceanography Distributed Active Archive Center (PO.DAAC)publisher_type :institutionpublisher_url :https://podaac.jpl.nasa.govreferences :ECCO Consortium, Fukumori, I., Wang, O., Fenty, I., Forget, G., Heimbach, P., & Ponte, R. M. 2020. Synopsis of the ECCO Central Production Global Ocean and Sea-Ice State Estimate (Version 4 Release 4). doi:10.5281/zenodo.3765928source :The ECCO V4r4 state estimate was produced by fitting a free-running solution of the MITgcm (checkpoint 66g) to satellite and in situ observational data in a least squares sense using the adjoint methodstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventionsummary :This dataset provides monthly-averaged dynamic sea surface height interpolated to a regular 0.5-degree grid from the ECCO Version 4 Release 4 (V4r4) ocean and sea-ice state estimate. Estimating the Circulation and Climate of the Ocean (ECCO) state estimates are dynamically and kinematically-consistent reconstructions of the three-dimensional, time-evolving ocean, sea-ice, and surface atmospheric states. ECCO V4r4 is a free-running solution of a global, nominally 1-degree configuration of the MIT general circulation model (MITgcm) that has been fit to observations in a least-squares sense. Observational data constraints used in V4r4 include sea surface height (SSH) from satellite altimeters [ERS-1/2, TOPEX/Poseidon, GFO, ENVISAT, Jason-1,2,3, CryoSat-2, and SARAL/AltiKa]; sea surface temperature (SST) from satellite radiometers [AVHRR], sea surface salinity (SSS) from the Aquarius satellite radiometer/scatterometer, ocean bottom pressure (OBP) from the GRACE satellite gravimeter; sea-ice concentration from satellite radiometers [SSM/I and SSMIS], and in-situ ocean temperature and salinity measured with conductivity-temperature-depth (CTD) sensors and expendable bathythermographs (XBTs) from several programs [e.g., WOCE, GO-SHIP, Argo, and others] and platforms [e.g., research vessels, gliders, moorings, ice-tethered profilers, and instrumented pinnipeds]. V4r4 covers the period 1992-01-01T12:00:00 to 2018-01-01T00:00:00.time_coverage_duration :P1Mtime_coverage_end :2015-02-01T00:00:00time_coverage_resolution :P1Mtime_coverage_start :2015-01-01T00:00:00title :ECCO Sea Surface Height - Monthly Mean 0.5 Degree (Version 4 Release 4)uuid :088d03b8-4158-11eb-876b-0cc47a3f47f1\n\n\nGet the SSH variable as an xarray dataarray\n\nssh_da = ssh_ds.SSH.load()\nssh_da\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'SSH' (time: 1, latitude: 360, longitude: 720)>\narray([[[        nan,         nan,         nan, ...,         nan,\n                 nan,         nan],\n        [        nan,         nan,         nan, ...,         nan,\n                 nan,         nan],\n        [        nan,         nan,         nan, ...,         nan,\n                 nan,         nan],\n        ...,\n        [-0.42037615, -0.42037615, -0.42037615, ..., -0.42037615,\n         -0.42037615, -0.42037615],\n        [-0.42635897, -0.42635897, -0.42635897, ..., -0.42635897,\n         -0.42635897, -0.42635897],\n        [-0.43566173, -0.43566173, -0.43566173, ..., -0.43566173,\n         -0.43566173, -0.43566173]]], dtype=float32)\nCoordinates:\n  * time       (time) datetime64[ns] 2015-01-16T12:00:00\n  * latitude   (latitude) float32 -89.75 -89.25 -88.75 ... 88.75 89.25 89.75\n  * longitude  (longitude) float32 -179.8 -179.2 -178.8 ... 178.8 179.2 179.8\nAttributes:\n    coverage_content_type:  modelResult\n    long_name:              Dynamic sea surface height anomaly\n    standard_name:          sea_surface_height_above_geoid\n    units:                  m\n    comment:                Dynamic sea surface height anomaly above the geoi...\n    valid_min:              -1.8805772066116333\n    valid_max:              1.4207719564437866xarray.DataArray'SSH'time: 1latitude: 360longitude: 720nan nan nan nan nan nan ... -0.4357 -0.4357 -0.4357 -0.4357 -0.4357array([[[        nan,         nan,         nan, ...,         nan,\n                 nan,         nan],\n        [        nan,         nan,         nan, ...,         nan,\n                 nan,         nan],\n        [        nan,         nan,         nan, ...,         nan,\n                 nan,         nan],\n        ...,\n        [-0.42037615, -0.42037615, -0.42037615, ..., -0.42037615,\n         -0.42037615, -0.42037615],\n        [-0.42635897, -0.42635897, -0.42635897, ..., -0.42635897,\n         -0.42635897, -0.42635897],\n        [-0.43566173, -0.43566173, -0.43566173, ..., -0.43566173,\n         -0.43566173, -0.43566173]]], dtype=float32)Coordinates: (3)time(time)datetime64[ns]2015-01-16T12:00:00axis :Tbounds :time_bndscoverage_content_type :coordinatelong_name :center time of averaging periodstandard_name :timearray(['2015-01-16T12:00:00.000000000'], dtype='datetime64[ns]')latitude(latitude)float32-89.75 -89.25 ... 89.25 89.75axis :Ybounds :latitude_bndscomment :uniform grid spacing from -89.75 to 89.75 by 0.5coverage_content_type :coordinatelong_name :latitude at grid cell centerstandard_name :latitudeunits :degrees_northarray([-89.75, -89.25, -88.75, ...,  88.75,  89.25,  89.75], dtype=float32)longitude(longitude)float32-179.8 -179.2 ... 179.2 179.8axis :Xbounds :longitude_bndscomment :uniform grid spacing from -179.75 to 179.75 by 0.5coverage_content_type :coordinatelong_name :longitude at grid cell centerstandard_name :longitudeunits :degrees_eastarray([-179.75, -179.25, -178.75, ...,  178.75,  179.25,  179.75],\n      dtype=float32)Attributes: (7)coverage_content_type :modelResultlong_name :Dynamic sea surface height anomalystandard_name :sea_surface_height_above_geoidunits :mcomment :Dynamic sea surface height anomaly above the geoid, suitable for comparisons with altimetry sea surface height data products that apply the inverse barometer (IB) correction. Note: SSH is calculated by correcting model sea level anomaly ETAN for three effects: a) global mean steric sea level changes related to density changes in the Boussinesq volume-conserving model (Greatbatch correction, see sterGloH), b) the inverted barometer (IB) effect (see SSHIBC) and c) sea level displacement due to sea-ice and snow pressure loading (see sIceLoad). SSH can be compared with the similarly-named SSH variable in previous ECCO products that did not include atmospheric pressure loading (e.g., Version 4 Release 3). Use SSHNOIBC for comparisons with altimetry data products that do NOT apply the IB correction.valid_min :-1.8805772066116333valid_max :1.4207719564437866\n\n\nPlot the SSH dataarray for time 2015-01-16T12:00:00 using hvplot.\n\nssh_da.hvplot.image(x='longitude', y='latitude', cmap='Spectral_r', aspect='equal').opts(clim=(ssh_da.attrs['valid_min'],ssh_da.attrs['valid_max']))\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nDirect access to ECCO data in S3 (from us-west-2)\nData_Access__Direct_S3_Access__PODAAC_ECCO_SSH using CMR-STAC API to retrieve S3 links"
  },
  {
    "objectID": "notebooks/Data_Access_and_Processing/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html",
    "href": "notebooks/Data_Access_and_Processing/Earthdata_Cloud__Single_File__Direct_S3_Access_Clip_COG_Example.html",
    "title": "2022 LP DAAC UWG Cloud Workshop",
    "section": "",
    "text": "In this notebook, we will access data for the Harmonized Landsat Sentinel-2 (HLS) Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0 (L30) (10.5067/HLS/HLSL30.002) data product. These data are archived and distributed as Cloud Optimized GeoTIFF (COG) files, one file for each spectral band.\nWe will access a single COG file, L30 red band (0.64 – 0.67 μm), from inside the AWS cloud (us-west-2 region, specifically) and load it into Python as an xarray dataarray. This approach leverages S3 native protocols for efficient access to the data.\n\n\n\n\n\nNASA Earthdata Cloud data in S3 can be directly accessed via temporary credentials; this access is limited to requests made within the US West (Oregon) (code: us-west-2) AWS region.\n\n\n\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. For additional information see: Authentication for NASA Earthdata.\n\n\n\n\n\nhow to retrieve temporary S3 credentials for in-region direct S3 bucket access\nhow to perform in-region direct access of HLS Cloud Optimized geoTIFF (COG) files in S3\nhot to clip data to region of interest\nhow to plot the data\n\n\n\n\n\nimport os\nimport requests \nimport boto3\nfrom osgeo import gdal\nimport rasterio as rio\nfrom rasterio.session import AWSSession\nimport rioxarray\nimport geopandas\nfrom shapely.geometry import Polygon\nfrom shapely.ops import transform\nimport pyproj\nfrom pyproj import Proj\nimport hvplot.xarray\nimport holoviews as hv\nimport geoviews as gv\ngv.extension('bokeh', 'matplotlib')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n  \n  \n\n\n\n\n\n\n\n\n\nDirect S3 access is achieved by passing NASA supplied temporary credentials to AWS so we can interact with S3 objects from applicable Earthdata Cloud buckets. For now, each NASA DAAC has different AWS credentials endpoints. Below are some of the credential endpoints to various DAACs:\n\ns3_cred_endpoint = {\n    'podaac':'https://archive.podaac.earthdata.nasa.gov/s3credentials',\n    'gesdisc': 'https://data.gesdisc.earthdata.nasa.gov/s3credentials',\n    'lpdaac':'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials',\n    'ornldaac': 'https://data.ornldaac.earthdata.nasa.gov/s3credentials',\n    'ghrcdaac': 'https://data.ghrc.earthdata.nasa.gov/s3credentials'\n}\n\nCreate a function to make a request to an endpoint for temporary credentials. Remember, each DAAC has their own endpoint and credentials are not usable for cloud data from other DAACs.\n\ndef get_temp_creds(provider):\n    return requests.get(s3_cred_endpoint[provider]).json()\n\n\ntemp_creds_req = get_temp_creds('lpdaac')\n#temp_creds_req\n\n\n\n\nFor this exercise, we are going to open up a context manager for the notebook using the rasterio.env module to store the required GDAL and AWS configurations we need to access the data in Earthdata Cloud. While the context manager is open (rio_env.__enter__()) we will be able to run the open or get data commands that would typically be executed within a with statement, thus allowing us to more freely interact with the data. We’ll close the context (rio_env.__exit__()) at the end of the notebook.\nCreate a boto3 Session object using your temporary credentials. This Session is used to pass credentials and configuration to AWS so we can interact wit S3 objects from applicable buckets.\n\nsession = boto3.Session(aws_access_key_id=temp_creds_req['accessKeyId'], \n                        aws_secret_access_key=temp_creds_req['secretAccessKey'],\n                        aws_session_token=temp_creds_req['sessionToken'],\n                        region_name='us-west-2')\n\nGDAL environment variables must be configured to access COGs in Earthdata Cloud. Geospatial data access Python packages like rasterio and rioxarray depend on GDAL, leveraging GDAL’s “Virtual File Systems” to read remote files. GDAL has a lot of environment variables that control it’s behavior. Changing these settings can mean the difference being able to access a file or not. They can also have an impact on the performance.\n\nrio_env = rio.Env(AWSSession(session),\n                  GDAL_DISABLE_READDIR_ON_OPEN='TRUE',\n                  GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n                  GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'))\nrio_env.__enter__()\n\n<rasterio.env.Env at 0x7f4c49384be0>\n\n\nIn this example we’re interested in the HLS L30 data collection from NASA’s LP DAAC in Earthdata Cloud. Below we specify the S3 URL to the data asset in Earthdata Cloud. This URL can be found via Earthdata Search or programmatically through the CMR and CMR-STAC APIs.\n\ns3_url = 's3://lp-prod-protected/HLSL30.020/HLS.L30.T10SGD.2020272T183449.v2.0/HLS.L30.T10SGD.2020272T183449.v2.0.B04.tif'\n\n\n\n\nRead in the HLS S3 URL for the L30 red band (0.64 – 0.67 μm) into our workspace using rioxarray, an extension of xarray used to read geospatial data.\n\nda = rioxarray.open_rasterio(s3_url, chunks='auto')\nda\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (band: 1, y: 3660, x: 3660)>\ndask.array<open_rasterio-4c950b483dfb0e3700f71e01d27c6a99<this-array>, shape=(1, 3660, 3660), dtype=int16, chunksize=(1, 3660, 3660), chunktype=numpy.ndarray>\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 7e+05 7e+05 7e+05 ... 8.097e+05 8.097e+05 8.097e+05\n  * y            (y) float64 3.9e+06 3.9e+06 3.9e+06 ... 3.79e+06 3.79e+06\n    spatial_ref  int64 0\nAttributes:\n    _FillValue:    -9999.0\n    scale_factor:  0.0001\n    add_offset:    0.0\n    long_name:     Redxarray.DataArrayband: 1y: 3660x: 3660dask.array<chunksize=(1, 3660, 3660), meta=np.ndarray>\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         25.55 MiB \n                         25.55 MiB \n                    \n                    \n                    \n                         Shape \n                         (1, 3660, 3660) \n                         (1, 3660, 3660) \n                    \n                    \n                         Count \n                         2 Tasks \n                         1 Chunks \n                    \n                    \n                     Type \n                     int16 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  3660\n  3660\n  1\n\n        \n    \nCoordinates: (4)band(band)int641array([1])x(x)float647e+05 7e+05 ... 8.097e+05 8.097e+05array([699975., 700005., 700035., ..., 809685., 809715., 809745.])y(y)float643.9e+06 3.9e+06 ... 3.79e+06array([3899985., 3899955., 3899925., ..., 3790275., 3790245., 3790215.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 10, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-123],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 10, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-123.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 10, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-123],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 3900000.0 0.0 -30.0array(0)Attributes: (4)_FillValue :-9999.0scale_factor :0.0001add_offset :0.0long_name :Red\n\n\nThe file is read into Python as an xarray dataarray with a band, x, and y dimension. In this example the band dimension is meaningless, so we’ll use the squeeze() function to remove band as a dimension.\n\nda = da.squeeze('band', drop=True)\nda\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (y: 3660, x: 3660)>\ndask.array<getitem, shape=(3660, 3660), dtype=int16, chunksize=(3660, 3660), chunktype=numpy.ndarray>\nCoordinates:\n  * x            (x) float64 7e+05 7e+05 7e+05 ... 8.097e+05 8.097e+05 8.097e+05\n  * y            (y) float64 3.9e+06 3.9e+06 3.9e+06 ... 3.79e+06 3.79e+06\n    spatial_ref  int64 0\nAttributes:\n    _FillValue:    -9999.0\n    scale_factor:  0.0001\n    add_offset:    0.0\n    long_name:     Redxarray.DataArrayy: 3660x: 3660dask.array<chunksize=(3660, 3660), meta=np.ndarray>\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         25.55 MiB \n                         25.55 MiB \n                    \n                    \n                    \n                         Shape \n                         (3660, 3660) \n                         (3660, 3660) \n                    \n                    \n                         Count \n                         3 Tasks \n                         1 Chunks \n                    \n                    \n                     Type \n                     int16 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  3660\n  3660\n\n        \n    \nCoordinates: (3)x(x)float647e+05 7e+05 ... 8.097e+05 8.097e+05array([699975., 700005., 700035., ..., 809685., 809715., 809745.])y(y)float643.9e+06 3.9e+06 ... 3.79e+06array([3899985., 3899955., 3899925., ..., 3790275., 3790245., 3790215.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 10, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-123],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 10, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-123.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 10, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-123],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 3900000.0 0.0 -30.0array(0)Attributes: (4)_FillValue :-9999.0scale_factor :0.0001add_offset :0.0long_name :Red\n\n\nPlot the dataarray, representing the L30 red band, using hvplot.\n\nda.rio.reproject('EPSG:3857').hvplot.image(x = 'x', y = 'y', cmap='gray', rasterize=False, width=800, height=600, tiles='EsriImagery', colorbar=True)\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\nWe’ll read in our GeoJSON file of our points of interest and create bounding box that contains a points coordinates\n\npoints = field = geopandas.read_file('../../data/TNC_fall_2020.geojson')\n\nExtract the min/max values for the y and x axis\n\nminx, miny, maxx, maxy = points.geometry.total_bounds\nminx, miny, maxx, maxy\n\n(-120.45264628215773,\n 34.51050622261265,\n -120.40432447545712,\n 34.532398755692206)\n\n\nOrder the coordinates for the bounding box counterclockwise\n\ncoords = [\n    (minx, miny),\n    (maxx, miny),\n    (maxx, maxy),\n    (minx, maxy)\n]\n\nCreate a shapely polygon\n\nfeature_shape = Polygon(coords)\nfeature_shape\n\n\n\n\n\nbase = gv.tile_sources.EsriImagery.opts(width=650, height=500)\nfarmField = gv.Polygons(feature_shape).opts(line_color='yellow', line_width=10, color=None)\nbase * farmField\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\nLet’s take a look at the bounding coordinate values.\nNote, the values above are in decimal degrees and represent the longitude and latitude for the lower left corner and upper right corner respectively.\n\nfeature_shape.bounds\n\n(-120.45264628215773,\n 34.51050622261265,\n -120.40432447545712,\n 34.532398755692206)\n\n\nGet the projection information from the HLS file\n\nsrc_proj = da.rio.crs\nsrc_proj\n\nCRS.from_wkt('PROJCS[\"UTM Zone 10, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-123],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]')\n\n\nTransform coordinates from lat lon (units = dd) to UTM (units = m)\n\ngeo_CRS = Proj('+proj=longlat +datum=WGS84 +no_defs', preserve_units=True)   # Source coordinate system of the ROI\n\n\nproject = pyproj.Transformer.from_proj(geo_CRS, src_proj)                    # Set up the transformation\n\n\nfsUTM = transform(project.transform, feature_shape)\nfsUTM.bounds\n\n(733792.5759279637, 3821708.6775865103, 738290.9586659562, 3824250.087197832)\n\n\nThe coordinates for our feature have now been converted to source raster projection. Note the difference in the values between feature_shape.bounds (in geographic) and fsUTM.bounds (in UTM projection).\nNow we can clip our COG file to our region of insterest!\n\n\n\nWe can now use our transformed ROI bounding box to clip the HLS S3 object we accessed before. We’ll use the rio.clip\n\nda_clip = rioxarray.open_rasterio(s3_url, chunks='auto').squeeze('band', drop=True).rio.clip([fsUTM])\n\n\nda_clip\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (y: 85, x: 150)>\ndask.array<astype, shape=(85, 150), dtype=int16, chunksize=(85, 150), chunktype=numpy.ndarray>\nCoordinates:\n  * y            (y) float64 3.824e+06 3.824e+06 ... 3.822e+06 3.822e+06\n  * x            (x) float64 7.338e+05 7.338e+05 ... 7.383e+05 7.383e+05\n    spatial_ref  int64 0\nAttributes:\n    scale_factor:  0.0001\n    add_offset:    0.0\n    long_name:     Red\n    _FillValue:    -9999xarray.DataArrayy: 85x: 150dask.array<chunksize=(85, 150), meta=np.ndarray>\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         24.90 kiB \n                         24.90 kiB \n                    \n                    \n                    \n                         Shape \n                         (85, 150) \n                         (85, 150) \n                    \n                    \n                         Count \n                         12 Tasks \n                         1 Chunks \n                    \n                    \n                     Type \n                     int16 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  150\n  85\n\n        \n    \nCoordinates: (3)y(y)float643.824e+06 3.824e+06 ... 3.822e+06axis :Ylong_name :y coordinate of projectionstandard_name :projection_y_coordinateunits :metrearray([3824235., 3824205., 3824175., 3824145., 3824115., 3824085., 3824055.,\n       3824025., 3823995., 3823965., 3823935., 3823905., 3823875., 3823845.,\n       3823815., 3823785., 3823755., 3823725., 3823695., 3823665., 3823635.,\n       3823605., 3823575., 3823545., 3823515., 3823485., 3823455., 3823425.,\n       3823395., 3823365., 3823335., 3823305., 3823275., 3823245., 3823215.,\n       3823185., 3823155., 3823125., 3823095., 3823065., 3823035., 3823005.,\n       3822975., 3822945., 3822915., 3822885., 3822855., 3822825., 3822795.,\n       3822765., 3822735., 3822705., 3822675., 3822645., 3822615., 3822585.,\n       3822555., 3822525., 3822495., 3822465., 3822435., 3822405., 3822375.,\n       3822345., 3822315., 3822285., 3822255., 3822225., 3822195., 3822165.,\n       3822135., 3822105., 3822075., 3822045., 3822015., 3821985., 3821955.,\n       3821925., 3821895., 3821865., 3821835., 3821805., 3821775., 3821745.,\n       3821715.])x(x)float647.338e+05 7.338e+05 ... 7.383e+05axis :Xlong_name :x coordinate of projectionstandard_name :projection_x_coordinateunits :metrearray([733815., 733845., 733875., 733905., 733935., 733965., 733995., 734025.,\n       734055., 734085., 734115., 734145., 734175., 734205., 734235., 734265.,\n       734295., 734325., 734355., 734385., 734415., 734445., 734475., 734505.,\n       734535., 734565., 734595., 734625., 734655., 734685., 734715., 734745.,\n       734775., 734805., 734835., 734865., 734895., 734925., 734955., 734985.,\n       735015., 735045., 735075., 735105., 735135., 735165., 735195., 735225.,\n       735255., 735285., 735315., 735345., 735375., 735405., 735435., 735465.,\n       735495., 735525., 735555., 735585., 735615., 735645., 735675., 735705.,\n       735735., 735765., 735795., 735825., 735855., 735885., 735915., 735945.,\n       735975., 736005., 736035., 736065., 736095., 736125., 736155., 736185.,\n       736215., 736245., 736275., 736305., 736335., 736365., 736395., 736425.,\n       736455., 736485., 736515., 736545., 736575., 736605., 736635., 736665.,\n       736695., 736725., 736755., 736785., 736815., 736845., 736875., 736905.,\n       736935., 736965., 736995., 737025., 737055., 737085., 737115., 737145.,\n       737175., 737205., 737235., 737265., 737295., 737325., 737355., 737385.,\n       737415., 737445., 737475., 737505., 737535., 737565., 737595., 737625.,\n       737655., 737685., 737715., 737745., 737775., 737805., 737835., 737865.,\n       737895., 737925., 737955., 737985., 738015., 738045., 738075., 738105.,\n       738135., 738165., 738195., 738225., 738255., 738285.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 10, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-123],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 10, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-123.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 10, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-123],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :733800.0 30.0 0.0 3824250.0 0.0 -30.0array(0)Attributes: (4)scale_factor :0.0001add_offset :0.0long_name :Red_FillValue :-9999\n\n\n\nda_clip.rio.reproject(\"EPSG:3857\").hvplot.image(x = 'x', y = 'y', cmap='gray', rasterize=False, width=800, height=600, tiles='EsriImagery', colorbar=True)\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\nExit the context manager.\n\nrio_env.__exit__()\n\n\n\n\n\nDirect S3 Data Access with rioxarray\nDirect_S3_Access__gdalvrt\nDirect_S3_Access__rioxarray_clipping\nGetting Started with Cloud-Native Harmonized Landsat Sentinel-2 (HLS) Data in R"
  },
  {
    "objectID": "notebooks/Data_Access_and_Processing/Multi-File_Direct_S3_Access_NetCDF_Example.html",
    "href": "notebooks/Data_Access_and_Processing/Multi-File_Direct_S3_Access_NetCDF_Example.html",
    "title": "2022 LP DAAC UWG Cloud Workshop",
    "section": "",
    "text": "In this notebook, we will access monthly sea surface height from ECCO V4r4 (10.5067/ECG5D-SSH44). The data are provided as a time series of monthly netCDFs on a 0.5-degree latitude/longitude grid.\nWe will access the data from inside the AWS cloud (us-west-2 region, specifically) and load a time series made of multiple netCDF datasets into an xarray dataset. This approach leverages S3 native protocols for efficient access to the data.\n\n\n\n\n\nNASA Earthdata Cloud data in S3 can be directly accessed via temporary credentials; this access is limited to requests made within the US West (Oregon) (code: us-west-2) AWS region.\n\n\n\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. For additional information see: Authentication for NASA Earthdata.\n\n\n\n\n\nhow to retrieve temporary S3 credentials for in-region direct S3 bucket access\nhow to define a dataset of interest and find netCDF files in S3 bucket\nhow to perform in-region direct access of ECCO_L4_SSH_05DEG_MONTHLY_V4R4 data in S3\nhow to plot the data\n\n\n\n\n\n\nimport os\nimport requests\nimport s3fs\nimport xarray as xr\nimport hvplot.xarray\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDirect S3 access is achieved by passing NASA supplied temporary credentials to AWS so we can interact with S3 objects from applicable Earthdata Cloud buckets. For now, each NASA DAAC has different AWS credentials endpoints. Below are some of the credential endpoints to various DAACs:\n\ns3_cred_endpoint = {\n    'podaac':'https://archive.podaac.earthdata.nasa.gov/s3credentials',\n    'gesdisc': 'https://data.gesdisc.earthdata.nasa.gov/s3credentials',\n    'lpdaac':'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials',\n    'ornldaac': 'https://data.ornldaac.earthdata.nasa.gov/s3credentials',\n    'ghrcdaac': 'https://data.ghrc.earthdata.nasa.gov/s3credentials'\n}\n\nCreate a function to make a request to an endpoint for temporary credentials. Remember, each DAAC has their own endpoint and credentials are not usable for cloud data from other DAACs.\n\ndef get_temp_creds(provider):\n    return requests.get(s3_cred_endpoint[provider]).json()\n\n\ntemp_creds_req = get_temp_creds('podaac')\n#temp_creds_req\n\n\n\n\ns3fs sessions are used for authenticated access to s3 bucket and allows for typical file-system style operations. Below we create session by passing in the temporary credentials we recieved from our temporary credentials endpoint.\n\nfs_s3 = s3fs.S3FileSystem(anon=False, \n                          key=temp_creds_req['accessKeyId'], \n                          secret=temp_creds_req['secretAccessKey'], \n                          token=temp_creds_req['sessionToken'],\n                          client_kwargs={'region_name':'us-west-2'})\n\nIn this example we’re interested in the ECCO data collection from NASA’s PO.DAAC in Earthdata Cloud. In this case it’s the following string that unique identifies the collection of monthly, 0.5-degree sea surface height data (ECCO_L4_SSH_05DEG_MONTHLY_V4R4).\n\nshort_name = 'ECCO_L4_SSH_05DEG_MONTHLY_V4R4'\n\n\nbucket = os.path.join('podaac-ops-cumulus-protected/', short_name, '*2015*.nc')\nbucket\n\n'podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/*2015*.nc'\n\n\nGet a list of netCDF files located at the S3 path corresponding to the ECCO V4r4 monthly sea surface height dataset on the 0.5-degree latitude/longitude grid, for year 2015.\n\nssh_files = fs_s3.glob(bucket)\nssh_files\n\n['podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-01_ECCO_V4r4_latlon_0p50deg.nc',\n 'podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-02_ECCO_V4r4_latlon_0p50deg.nc',\n 'podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-03_ECCO_V4r4_latlon_0p50deg.nc',\n 'podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-04_ECCO_V4r4_latlon_0p50deg.nc',\n 'podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-05_ECCO_V4r4_latlon_0p50deg.nc',\n 'podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-06_ECCO_V4r4_latlon_0p50deg.nc',\n 'podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-07_ECCO_V4r4_latlon_0p50deg.nc',\n 'podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-08_ECCO_V4r4_latlon_0p50deg.nc',\n 'podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-09_ECCO_V4r4_latlon_0p50deg.nc',\n 'podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-10_ECCO_V4r4_latlon_0p50deg.nc',\n 'podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-11_ECCO_V4r4_latlon_0p50deg.nc',\n 'podaac-ops-cumulus-protected/ECCO_L4_SSH_05DEG_MONTHLY_V4R4/SEA_SURFACE_HEIGHT_mon_mean_2015-12_ECCO_V4r4_latlon_0p50deg.nc']\n\n\n\n\n\nOpen with the netCDF files using the s3fs package, then load them all at once into a concatenated xarray dataset.\n\nfileset = [fs_s3.open(file) for file in ssh_files]\n\nCreate an xarray dataset using the open_mfdataset() function to “read in” all of the netCDF4 files in one call.\n\nssh_ds = xr.open_mfdataset(fileset,\n                           combine='by_coords',\n                           mask_and_scale=True,\n                           decode_cf=True,\n                           chunks='auto')\nssh_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:         (time: 12, latitude: 360, longitude: 720, nv: 2)\nCoordinates:\n  * time            (time) datetime64[ns] 2015-01-16T12:00:00 ... 2015-12-16T...\n  * latitude        (latitude) float32 -89.75 -89.25 -88.75 ... 89.25 89.75\n  * longitude       (longitude) float32 -179.8 -179.2 -178.8 ... 179.2 179.8\n    time_bnds       (time, nv) datetime64[ns] dask.array<chunksize=(1, 2), meta=np.ndarray>\n    latitude_bnds   (latitude, nv) float32 dask.array<chunksize=(360, 2), meta=np.ndarray>\n    longitude_bnds  (longitude, nv) float32 dask.array<chunksize=(720, 2), meta=np.ndarray>\nDimensions without coordinates: nv\nData variables:\n    SSH             (time, latitude, longitude) float32 dask.array<chunksize=(1, 360, 720), meta=np.ndarray>\n    SSHIBC          (time, latitude, longitude) float32 dask.array<chunksize=(1, 360, 720), meta=np.ndarray>\n    SSHNOIBC        (time, latitude, longitude) float32 dask.array<chunksize=(1, 360, 720), meta=np.ndarray>\nAttributes: (12/57)\n    acknowledgement:              This research was carried out by the Jet Pr...\n    author:                       Ian Fenty and Ou Wang\n    cdm_data_type:                Grid\n    comment:                      Fields provided on a regular lat-lon grid. ...\n    Conventions:                  CF-1.8, ACDD-1.3\n    coordinates_comment:          Note: the global 'coordinates' attribute de...\n    ...                           ...\n    time_coverage_duration:       P1M\n    time_coverage_end:            2015-02-01T00:00:00\n    time_coverage_resolution:     P1M\n    time_coverage_start:          2015-01-01T00:00:00\n    title:                        ECCO Sea Surface Height - Monthly Mean 0.5 ...\n    uuid:                         088d03b8-4158-11eb-876b-0cc47a3f47f1xarray.DatasetDimensions:time: 12latitude: 360longitude: 720nv: 2Coordinates: (6)time(time)datetime64[ns]2015-01-16T12:00:00 ... 2015-12-...axis :Tbounds :time_bndscoverage_content_type :coordinatelong_name :center time of averaging periodstandard_name :timearray(['2015-01-16T12:00:00.000000000', '2015-02-15T00:00:00.000000000',\n       '2015-03-16T12:00:00.000000000', '2015-04-16T00:00:00.000000000',\n       '2015-05-16T12:00:00.000000000', '2015-06-16T00:00:00.000000000',\n       '2015-07-16T12:00:00.000000000', '2015-08-16T12:00:00.000000000',\n       '2015-09-16T00:00:00.000000000', '2015-10-16T12:00:00.000000000',\n       '2015-11-16T00:00:00.000000000', '2015-12-16T12:00:00.000000000'],\n      dtype='datetime64[ns]')latitude(latitude)float32-89.75 -89.25 ... 89.25 89.75axis :Ybounds :latitude_bndscomment :uniform grid spacing from -89.75 to 89.75 by 0.5coverage_content_type :coordinatelong_name :latitude at grid cell centerstandard_name :latitudeunits :degrees_northarray([-89.75, -89.25, -88.75, ...,  88.75,  89.25,  89.75], dtype=float32)longitude(longitude)float32-179.8 -179.2 ... 179.2 179.8axis :Xbounds :longitude_bndscomment :uniform grid spacing from -179.75 to 179.75 by 0.5coverage_content_type :coordinatelong_name :longitude at grid cell centerstandard_name :longitudeunits :degrees_eastarray([-179.75, -179.25, -178.75, ...,  178.75,  179.25,  179.75],\n      dtype=float32)time_bnds(time, nv)datetime64[ns]dask.array<chunksize=(1, 2), meta=np.ndarray>comment :Start and end times of averaging period.coverage_content_type :coordinatelong_name :time bounds of averaging period\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         192 B \n                         16 B \n                    \n                    \n                    \n                         Shape \n                         (12, 2) \n                         (1, 2) \n                    \n                    \n                         Count \n                         36 Tasks \n                         12 Chunks \n                    \n                    \n                     Type \n                     datetime64[ns] \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  2\n  12\n\n        \n    \nlatitude_bnds(latitude, nv)float32dask.array<chunksize=(360, 2), meta=np.ndarray>coverage_content_type :coordinatelong_name :latitude bounds grid cells\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         2.81 kiB \n                         2.81 kiB \n                    \n                    \n                    \n                         Shape \n                         (360, 2) \n                         (360, 2) \n                    \n                    \n                         Count \n                         55 Tasks \n                         1 Chunks \n                    \n                    \n                     Type \n                     float32 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  2\n  360\n\n        \n    \nlongitude_bnds(longitude, nv)float32dask.array<chunksize=(720, 2), meta=np.ndarray>coverage_content_type :coordinatelong_name :longitude bounds grid cells\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         5.62 kiB \n                         5.62 kiB \n                    \n                    \n                    \n                         Shape \n                         (720, 2) \n                         (720, 2) \n                    \n                    \n                         Count \n                         55 Tasks \n                         1 Chunks \n                    \n                    \n                     Type \n                     float32 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  2\n  720\n\n        \n    \nData variables: (3)SSH(time, latitude, longitude)float32dask.array<chunksize=(1, 360, 720), meta=np.ndarray>coverage_content_type :modelResultlong_name :Dynamic sea surface height anomalystandard_name :sea_surface_height_above_geoidunits :mcomment :Dynamic sea surface height anomaly above the geoid, suitable for comparisons with altimetry sea surface height data products that apply the inverse barometer (IB) correction. Note: SSH is calculated by correcting model sea level anomaly ETAN for three effects: a) global mean steric sea level changes related to density changes in the Boussinesq volume-conserving model (Greatbatch correction, see sterGloH), b) the inverted barometer (IB) effect (see SSHIBC) and c) sea level displacement due to sea-ice and snow pressure loading (see sIceLoad). SSH can be compared with the similarly-named SSH variable in previous ECCO products that did not include atmospheric pressure loading (e.g., Version 4 Release 3). Use SSHNOIBC for comparisons with altimetry data products that do NOT apply the IB correction.valid_min :-1.8805772066116333valid_max :1.4207719564437866\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         11.87 MiB \n                         0.99 MiB \n                    \n                    \n                    \n                         Shape \n                         (12, 360, 720) \n                         (1, 360, 720) \n                    \n                    \n                         Count \n                         36 Tasks \n                         12 Chunks \n                    \n                    \n                     Type \n                     float32 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  720\n  360\n  12\n\n        \n    \nSSHIBC(time, latitude, longitude)float32dask.array<chunksize=(1, 360, 720), meta=np.ndarray>coverage_content_type :modelResultlong_name :The inverted barometer (IB) correction to sea surface height due to atmospheric pressure loadingunits :mcomment :Not an SSH itself, but a correction to model sea level anomaly (ETAN) required to account for the static part of sea surface displacement by atmosphere pressure loading: SSH = SSHNOIBC - SSHIBC. Note: Use SSH for model-data comparisons with altimetry data products that DO apply the IB correction and SSHNOIBC for comparisons with altimetry data products that do NOT apply the IB correction.valid_min :-0.30144819617271423valid_max :0.5245633721351624\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         11.87 MiB \n                         0.99 MiB \n                    \n                    \n                    \n                         Shape \n                         (12, 360, 720) \n                         (1, 360, 720) \n                    \n                    \n                         Count \n                         36 Tasks \n                         12 Chunks \n                    \n                    \n                     Type \n                     float32 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  720\n  360\n  12\n\n        \n    \nSSHNOIBC(time, latitude, longitude)float32dask.array<chunksize=(1, 360, 720), meta=np.ndarray>coverage_content_type :modelResultlong_name :Sea surface height anomaly without the inverted barometer (IB) correctionunits :mcomment :Sea surface height anomaly above the geoid without the inverse barometer (IB) correction, suitable for comparisons with altimetry sea surface height data products that do NOT apply the inverse barometer (IB) correction. Note: SSHNOIBC is calculated by correcting model sea level anomaly ETAN for two effects: a) global mean steric sea level changes related to density changes in the Boussinesq volume-conserving model (Greatbatch correction, see sterGloH), b) sea level displacement due to sea-ice and snow pressure loading (see sIceLoad). In ECCO Version 4 Release 4 the model is forced with atmospheric pressure loading. SSHNOIBC does not correct for the static part of the effect of atmosphere pressure loading on sea surface height (the so-called inverse barometer (IB) correction). Use SSH for comparisons with altimetry data products that DO apply the IB correction.valid_min :-1.6654272079467773valid_max :1.4550364017486572\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         11.87 MiB \n                         0.99 MiB \n                    \n                    \n                    \n                         Shape \n                         (12, 360, 720) \n                         (1, 360, 720) \n                    \n                    \n                         Count \n                         36 Tasks \n                         12 Chunks \n                    \n                    \n                     Type \n                     float32 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  720\n  360\n  12\n\n        \n    \nAttributes: (57)acknowledgement :This research was carried out by the Jet Propulsion Laboratory, managed by the California Institute of Technology under a contract with the National Aeronautics and Space Administration.author :Ian Fenty and Ou Wangcdm_data_type :Gridcomment :Fields provided on a regular lat-lon grid. They have been mapped to the regular lat-lon grid from the original ECCO lat-lon-cap 90 (llc90) native model grid. SSH (dynamic sea surface height) = SSHNOIBC (dynamic sea surface without the inverse barometer correction) - SSHIBC (inverse barometer correction). The inverted barometer correction accounts for variations in sea surface height due to atmospheric pressure variations.Conventions :CF-1.8, ACDD-1.3coordinates_comment :Note: the global 'coordinates' attribute describes auxillary coordinates.creator_email :ecco-group@mit.educreator_institution :NASA Jet Propulsion Laboratory (JPL)creator_name :ECCO Consortiumcreator_type :groupcreator_url :https://ecco-group.orgdate_created :2020-12-18T09:39:51date_issued :2020-12-18T09:39:51date_metadata_modified :2021-03-15T22:07:49date_modified :2021-03-15T22:07:49geospatial_bounds_crs :EPSG:4326geospatial_lat_max :90.0geospatial_lat_min :-90.0geospatial_lat_resolution :0.5geospatial_lat_units :degrees_northgeospatial_lon_max :180.0geospatial_lon_min :-180.0geospatial_lon_resolution :0.5geospatial_lon_units :degrees_easthistory :Inaugural release of an ECCO Central Estimate solution to PO.DAACid :10.5067/ECG5M-SSH44institution :NASA Jet Propulsion Laboratory (JPL)instrument_vocabulary :GCMD instrument keywordskeywords :EARTH SCIENCE > OCEANS > SEA SURFACE TOPOGRAPHY > SEA SURFACE HEIGHT, EARTH SCIENCE SERVICES > MODELS > EARTH SCIENCE REANALYSES/ASSIMILATION MODELSkeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordslicense :Public Domainmetadata_link :https://cmr.earthdata.nasa.gov/search/collections.umm_json?ShortName=ECCO_L4_SSH_05DEG_MONTHLY_V4R4naming_authority :gov.nasa.jplplatform :ERS-1/2, TOPEX/Poseidon, Geosat Follow-On (GFO), ENVISAT, Jason-1, Jason-2, CryoSat-2, SARAL/AltiKa, Jason-3, AVHRR, Aquarius, SSM/I, SSMIS, GRACE, DTU17MDT, Argo, WOCE, GO-SHIP, MEOP, Ice Tethered Profilers (ITP)platform_vocabulary :GCMD platform keywordsprocessing_level :L4product_name :SEA_SURFACE_HEIGHT_mon_mean_2015-01_ECCO_V4r4_latlon_0p50deg.ncproduct_time_coverage_end :2018-01-01T00:00:00product_time_coverage_start :1992-01-01T12:00:00product_version :Version 4, Release 4program :NASA Physical Oceanography, Cryosphere, Modeling, Analysis, and Prediction (MAP)project :Estimating the Circulation and Climate of the Ocean (ECCO)publisher_email :podaac@podaac.jpl.nasa.govpublisher_institution :PO.DAACpublisher_name :Physical Oceanography Distributed Active Archive Center (PO.DAAC)publisher_type :institutionpublisher_url :https://podaac.jpl.nasa.govreferences :ECCO Consortium, Fukumori, I., Wang, O., Fenty, I., Forget, G., Heimbach, P., & Ponte, R. M. 2020. Synopsis of the ECCO Central Production Global Ocean and Sea-Ice State Estimate (Version 4 Release 4). doi:10.5281/zenodo.3765928source :The ECCO V4r4 state estimate was produced by fitting a free-running solution of the MITgcm (checkpoint 66g) to satellite and in situ observational data in a least squares sense using the adjoint methodstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventionsummary :This dataset provides monthly-averaged dynamic sea surface height interpolated to a regular 0.5-degree grid from the ECCO Version 4 Release 4 (V4r4) ocean and sea-ice state estimate. Estimating the Circulation and Climate of the Ocean (ECCO) state estimates are dynamically and kinematically-consistent reconstructions of the three-dimensional, time-evolving ocean, sea-ice, and surface atmospheric states. ECCO V4r4 is a free-running solution of a global, nominally 1-degree configuration of the MIT general circulation model (MITgcm) that has been fit to observations in a least-squares sense. Observational data constraints used in V4r4 include sea surface height (SSH) from satellite altimeters [ERS-1/2, TOPEX/Poseidon, GFO, ENVISAT, Jason-1,2,3, CryoSat-2, and SARAL/AltiKa]; sea surface temperature (SST) from satellite radiometers [AVHRR], sea surface salinity (SSS) from the Aquarius satellite radiometer/scatterometer, ocean bottom pressure (OBP) from the GRACE satellite gravimeter; sea-ice concentration from satellite radiometers [SSM/I and SSMIS], and in-situ ocean temperature and salinity measured with conductivity-temperature-depth (CTD) sensors and expendable bathythermographs (XBTs) from several programs [e.g., WOCE, GO-SHIP, Argo, and others] and platforms [e.g., research vessels, gliders, moorings, ice-tethered profilers, and instrumented pinnipeds]. V4r4 covers the period 1992-01-01T12:00:00 to 2018-01-01T00:00:00.time_coverage_duration :P1Mtime_coverage_end :2015-02-01T00:00:00time_coverage_resolution :P1Mtime_coverage_start :2015-01-01T00:00:00title :ECCO Sea Surface Height - Monthly Mean 0.5 Degree (Version 4 Release 4)uuid :088d03b8-4158-11eb-876b-0cc47a3f47f1\n\n\nGet the SSH variable as an xarray dataarray\n\nssh_da = ssh_ds.SSH\nssh_da\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'SSH' (time: 12, latitude: 360, longitude: 720)>\ndask.array<concatenate, shape=(12, 360, 720), dtype=float32, chunksize=(1, 360, 720), chunktype=numpy.ndarray>\nCoordinates:\n  * time       (time) datetime64[ns] 2015-01-16T12:00:00 ... 2015-12-16T12:00:00\n  * latitude   (latitude) float32 -89.75 -89.25 -88.75 ... 88.75 89.25 89.75\n  * longitude  (longitude) float32 -179.8 -179.2 -178.8 ... 178.8 179.2 179.8\nAttributes:\n    coverage_content_type:  modelResult\n    long_name:              Dynamic sea surface height anomaly\n    standard_name:          sea_surface_height_above_geoid\n    units:                  m\n    comment:                Dynamic sea surface height anomaly above the geoi...\n    valid_min:              -1.8805772066116333\n    valid_max:              1.4207719564437866xarray.DataArray'SSH'time: 12latitude: 360longitude: 720dask.array<chunksize=(1, 360, 720), meta=np.ndarray>\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         11.87 MiB \n                         0.99 MiB \n                    \n                    \n                    \n                         Shape \n                         (12, 360, 720) \n                         (1, 360, 720) \n                    \n                    \n                         Count \n                         36 Tasks \n                         12 Chunks \n                    \n                    \n                     Type \n                     float32 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  720\n  360\n  12\n\n        \n    \nCoordinates: (3)time(time)datetime64[ns]2015-01-16T12:00:00 ... 2015-12-...axis :Tbounds :time_bndscoverage_content_type :coordinatelong_name :center time of averaging periodstandard_name :timearray(['2015-01-16T12:00:00.000000000', '2015-02-15T00:00:00.000000000',\n       '2015-03-16T12:00:00.000000000', '2015-04-16T00:00:00.000000000',\n       '2015-05-16T12:00:00.000000000', '2015-06-16T00:00:00.000000000',\n       '2015-07-16T12:00:00.000000000', '2015-08-16T12:00:00.000000000',\n       '2015-09-16T00:00:00.000000000', '2015-10-16T12:00:00.000000000',\n       '2015-11-16T00:00:00.000000000', '2015-12-16T12:00:00.000000000'],\n      dtype='datetime64[ns]')latitude(latitude)float32-89.75 -89.25 ... 89.25 89.75axis :Ybounds :latitude_bndscomment :uniform grid spacing from -89.75 to 89.75 by 0.5coverage_content_type :coordinatelong_name :latitude at grid cell centerstandard_name :latitudeunits :degrees_northarray([-89.75, -89.25, -88.75, ...,  88.75,  89.25,  89.75], dtype=float32)longitude(longitude)float32-179.8 -179.2 ... 179.2 179.8axis :Xbounds :longitude_bndscomment :uniform grid spacing from -179.75 to 179.75 by 0.5coverage_content_type :coordinatelong_name :longitude at grid cell centerstandard_name :longitudeunits :degrees_eastarray([-179.75, -179.25, -178.75, ...,  178.75,  179.25,  179.75],\n      dtype=float32)Attributes: (7)coverage_content_type :modelResultlong_name :Dynamic sea surface height anomalystandard_name :sea_surface_height_above_geoidunits :mcomment :Dynamic sea surface height anomaly above the geoid, suitable for comparisons with altimetry sea surface height data products that apply the inverse barometer (IB) correction. Note: SSH is calculated by correcting model sea level anomaly ETAN for three effects: a) global mean steric sea level changes related to density changes in the Boussinesq volume-conserving model (Greatbatch correction, see sterGloH), b) the inverted barometer (IB) effect (see SSHIBC) and c) sea level displacement due to sea-ice and snow pressure loading (see sIceLoad). SSH can be compared with the similarly-named SSH variable in previous ECCO products that did not include atmospheric pressure loading (e.g., Version 4 Release 3). Use SSHNOIBC for comparisons with altimetry data products that do NOT apply the IB correction.valid_min :-1.8805772066116333valid_max :1.4207719564437866\n\n\nPlot the SSH time series using hvplot\n\nssh_da.hvplot.image(y='latitude', x='longitude', cmap='Viridis',).opts(clim=(ssh_da.attrs['valid_min'],ssh_da.attrs['valid_max']))\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nDirect access to ECCO data in S3 (from us-west-2)\nData_Access__Direct_S3_Access__PODAAC_ECCO_SSH using CMR-STAC API to retrieve S3 links"
  },
  {
    "objectID": "notebooks/Data_Access_and_Processing/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html",
    "href": "notebooks/Data_Access_and_Processing/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html",
    "title": "2022 LP DAAC UWG Cloud Workshop",
    "section": "",
    "text": "In this notebook, we will access data for the Harmonized Landsat Sentinel-2 (HLS) Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0 (L30) (10.5067/HLS/HLSL30.002) data product. These data are archived and distributed as Cloud Optimized GeoTIFF (COG) files, one file for each spectral band.\nWe will access a single COG file, L30 red band (0.64 – 0.67 μm), from inside the AWS cloud (us-west-2 region, specifically) and load it into Python as an xarray dataarray. This approach leverages S3 native protocols for efficient access to the data.\n\n\n\n\n\nNASA Earthdata Cloud data in S3 can be directly accessed via temporary credentials; this access is limited to requests made within the US West (Oregon) (code: us-west-2) AWS region.\n\n\n\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n\nYou will need a netrc file containing your NASA Earthdata Login credentials in order to execute the notebooks. A netrc file can be created manually within text editor and saved to your home directory. For additional information see: Authentication for NASA Earthdata.\n\n\n\n\n\nhow to retrieve temporary S3 credentials for in-region direct S3 bucket access\nhow to perform in-region direct access of HLS Cloud Optimized geoTIFF (COG) files in S3\nhow to plot the data\n\n\n\n\n\nimport os\nimport requests \nimport boto3\nfrom osgeo import gdal\nimport rasterio as rio\nfrom rasterio.session import AWSSession\nimport rioxarray\nimport hvplot.xarray\nimport holoviews as hv\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDirect S3 access is achieved by passing NASA supplied temporary credentials to AWS so we can interact with S3 objects from applicable Earthdata Cloud buckets. For now, each NASA DAAC has different AWS credentials endpoints. Below are some of the credential endpoints to various DAACs:\n\ns3_cred_endpoint = {\n    'podaac':'https://archive.podaac.earthdata.nasa.gov/s3credentials',\n    'gesdisc': 'https://data.gesdisc.earthdata.nasa.gov/s3credentials',\n    'lpdaac':'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials',\n    'ornldaac': 'https://data.ornldaac.earthdata.nasa.gov/s3credentials',\n    'ghrcdaac': 'https://data.ghrc.earthdata.nasa.gov/s3credentials'\n}\n\nCreate a function to make a request to an endpoint for temporary credentials. Remember, each DAAC has their own endpoint and credentials are not usable for cloud data from other DAACs.\n\ndef get_temp_creds(provider):\n    return requests.get(s3_cred_endpoint[provider]).json()\n\n\ntemp_creds_req = get_temp_creds('lpdaac')\n#temp_creds_req\n\n\n\n\nFor this exercise, we are going to open up a context manager for the notebook using the rasterio.env module to store the required GDAL and AWS configurations we need to access the data in Earthdata Cloud. While the context manager is open (rio_env.__enter__()) we will be able to run the open or get data commands that would typically be executed within a with statement, thus allowing us to more freely interact with the data. We’ll close the context (rio_env.__exit__()) at the end of the notebook.\nCreate a boto3 Session object using your temporary credentials. This Session is used to pass credentials and configuration to AWS so we can interact wit S3 objects from applicable buckets.\n\nsession = boto3.Session(aws_access_key_id=temp_creds_req['accessKeyId'], \n                        aws_secret_access_key=temp_creds_req['secretAccessKey'],\n                        aws_session_token=temp_creds_req['sessionToken'],\n                        region_name='us-west-2')\n\nGDAL environment variables must be configured to access COGs in Earthdata Cloud. Geospatial data access Python packages like rasterio and rioxarray depend on GDAL, leveraging GDAL’s “Virtual File Systems” to read remote files. GDAL has a lot of environment variables that control it’s behavior. Changing these settings can mean the difference being able to access a file or not. They can also have an impact on the performance.\n\nrio_env = rio.Env(AWSSession(session),\n                  GDAL_DISABLE_READDIR_ON_OPEN='TRUE',\n                  GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n                  GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'))\nrio_env.__enter__()\n\n<rasterio.env.Env at 0x7fab5b226e20>\n\n\nIn this example we’re interested in the HLS L30 data collection from NASA’s LP DAAC in Earthdata Cloud. Below we specify the s3 URL to the data asset in Earthdata Cloud. This URL can be found via Earthdata Search or programmatically through the CMR and CMR-STAC APIs.\n\ns3_url = 's3://lp-prod-protected/HLSL30.020/HLS.L30.T11SQA.2021333T181532.v2.0/HLS.L30.T11SQA.2021333T181532.v2.0.B04.tif'\n\n\n\n\nRead in the HLS S3 URL for the L30 red band (0.64 – 0.67 μm) into our workspace using rioxarray, an extension of xarray used to read geospatial data.\n\nda = rioxarray.open_rasterio(s3_url)\nda\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (band: 1, y: 3660, x: 3660)>\n[13395600 values with dtype=int16]\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 7e+05 7e+05 7e+05 ... 8.097e+05 8.097e+05 8.097e+05\n  * y            (y) float64 4.1e+06 4.1e+06 4.1e+06 ... 3.99e+06 3.99e+06\n    spatial_ref  int64 0\nAttributes:\n    _FillValue:    -9999.0\n    scale_factor:  0.0001\n    add_offset:    0.0\n    long_name:     Redxarray.DataArrayband: 1y: 3660x: 3660...[13395600 values with dtype=int16]Coordinates: (4)band(band)int641array([1])x(x)float647e+05 7e+05 ... 8.097e+05 8.097e+05array([699975., 700005., 700035., ..., 809685., 809715., 809745.])y(y)float644.1e+06 4.1e+06 ... 3.99e+06array([4100025., 4099995., 4099965., ..., 3990315., 3990285., 3990255.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 11, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 11, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-117.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 11, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 4100040.0 0.0 -30.0array(0)Attributes: (4)_FillValue :-9999.0scale_factor :0.0001add_offset :0.0long_name :Red\n\n\nThe file is read into Python as an xarray dataarray with a band, x, and y dimension. In this example the band dimension is meaningless, so we’ll use the squeeze() function to remove band as a dimension.\n\nda_red = da.squeeze('band', drop=True)\nda_red\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (y: 3660, x: 3660)>\n[13395600 values with dtype=int16]\nCoordinates:\n  * x            (x) float64 7e+05 7e+05 7e+05 ... 8.097e+05 8.097e+05 8.097e+05\n  * y            (y) float64 4.1e+06 4.1e+06 4.1e+06 ... 3.99e+06 3.99e+06\n    spatial_ref  int64 0\nAttributes:\n    _FillValue:    -9999.0\n    scale_factor:  0.0001\n    add_offset:    0.0\n    long_name:     Redxarray.DataArrayy: 3660x: 3660...[13395600 values with dtype=int16]Coordinates: (3)x(x)float647e+05 7e+05 ... 8.097e+05 8.097e+05array([699975., 700005., 700035., ..., 809685., 809715., 809745.])y(y)float644.1e+06 4.1e+06 ... 3.99e+06array([4100025., 4099995., 4099965., ..., 3990315., 3990285., 3990255.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 11, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not_specified_based_on_WGS_84_spheroidprojected_crs_name :UTM Zone 11, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-117.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 11, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not_specified_based_on_WGS_84_spheroid\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :699960.0 30.0 0.0 4100040.0 0.0 -30.0array(0)Attributes: (4)_FillValue :-9999.0scale_factor :0.0001add_offset :0.0long_name :Red\n\n\nPlot the dataarray, representing the L30 red band, using hvplot.\n\nda_red.hvplot.image(x='x', y='y', cmap='gray', aspect='equal')\n\nUnable to display output for mime type(s): \n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nExit the context manager.\n\nrio_env.__exit__()\n\n\n\n\n\nDirect S3 Data Access with rioxarray\nDirect_S3_Access__gdalvrt\nDirect_S3_Access__rioxarray_clipping\nGetting Started with Cloud-Native Harmonized Landsat Sentinel-2 (HLS) Data in R"
  },
  {
    "objectID": "notebooks/Data_Access_and_Processing/Multi-File_Direct_S3_Access_COG_Example.html",
    "href": "notebooks/Data_Access_and_Processing/Multi-File_Direct_S3_Access_COG_Example.html",
    "title": "2022 LP DAAC UWG Cloud Workshop",
    "section": "",
    "text": "import os\nfrom osgeo import gdal\nimport pystac\nfrom pystac_client import Client\nimport rasterio as rio\nimport stackstac\nimport xarray as xr\nfrom dask import distributed\n\n\ndist_env = stackstac.DEFAULT_GDAL_ENV.updated(dict(\n    GDAL_DISABLE_READDIR_ON_OPEN='TRUE',\n    GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n    GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'))\n)\n\n\ncluster = distributed.LocalCluster()\nclient = distributed.Client(cluster)\nclient.dashboard_link\n\n'http://127.0.0.1:8787/status'\n\n\n\nSTAC_URL = 'https://cmr.earthdata.nasa.gov/stac'\n\n\ncatalog = Client.open(f\"{STAC_URL}/LPCLOUD\")\n\n\nsearch = catalog.search(\n    collections = ['HLSL30.v2.0', 'HLSS30.v2.0'],\n    intersects = {'type': 'Polygon',\n                  'coordinates': [[[-101.67271614074707, 41.04754380304359],\n                                   [-101.65344715118408, 41.04754380304359],\n                                   [-101.65344715118408, 41.06213891056728],\n                                   [-101.67271614074707, 41.06213891056728],\n                                   [-101.67271614074707, 41.04754380304359]]]},\n    datetime = '2021-05/2021-08'\n)               \n\n\nsearch.matched()\n\n113\n\n\n\nitem_col = search.get_all_items()\nitem_col\n\n<pystac.item_collection.ItemCollection at 0x7f917ab0b640>\n\n\n\ntile_item_col = [x for x in item_col if 'T13TGF' in x.id]\n\n\nitem_collection = pystac.ItemCollection(items=tile_item_col)\n\n\nitem_collection\n\n<pystac.item_collection.ItemCollection at 0x7f91792e3730>\n\n\n\ndata = stackstac.stack(item_collection, \n                       assets=['B04', 'B02'], \n                       epsg=32613, \n                       resolution=30,\n                       gdal_env=dist_env)\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'stackstac-616c3920d64d98e3874f76c6822be2c1' (time: 57,\n                                                                band: 2,\n                                                                y: 3911, x: 3915)>\ndask.array<fetch_raster_window, shape=(57, 2, 3911, 3915), dtype=float64, chunksize=(1, 1, 1024, 1024), chunktype=numpy.ndarray>\nCoordinates:\n  * time            (time) datetime64[ns] 2021-05-04T17:30:13.428000 ... 2021...\n    id              (time) <U34 'HLS.L30.T13TGF.2021124T173013.v2.0' ... 'HLS...\n  * band            (band) <U3 'B04' 'B02'\n  * x               (x) float64 6.97e+05 6.97e+05 ... 8.144e+05 8.144e+05\n  * y               (y) float64 4.604e+06 4.604e+06 ... 4.487e+06 4.487e+06\n    eo:cloud_cover  (time) int64 36 52 100 25 22 72 100 12 ... 0 37 0 30 58 58 0\n    end_datetime    (time) <U24 '2021-05-04T17:30:37.319Z' ... '2021-08-31T17...\n    start_datetime  (time) <U24 '2021-05-04T17:30:13.428Z' ... '2021-08-31T17...\n    epsg            int64 32613\nAttributes:\n    spec:        RasterSpec(epsg=32613, bounds=(696990, 4486590, 814440, 4603...\n    crs:         epsg:32613\n    transform:   | 30.00, 0.00, 696990.00|\\n| 0.00,-30.00, 4603920.00|\\n| 0.0...\n    resolution:  30xarray.DataArray'stackstac-616c3920d64d98e3874f76c6822be2c1'time: 57band: 2y: 3911x: 3915dask.array<chunksize=(1, 1, 1024, 1024), meta=np.ndarray>\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         13.01 GiB \n                         8.00 MiB \n                    \n                    \n                    \n                         Shape \n                         (57, 2, 3911, 3915) \n                         (1, 1, 1024, 1024) \n                    \n                    \n                         Count \n                         2052 Tasks \n                         1824 Chunks \n                    \n                    \n                     Type \n                     float64 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  57\n  1\n\n\n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  3915\n  3911\n  2\n\n        \n    \nCoordinates: (9)time(time)datetime64[ns]2021-05-04T17:30:13.428000 ... 2...array(['2021-05-04T17:30:13.428000000', '2021-05-05T17:42:36.240000000',\n       '2021-05-08T17:52:32.807000000', '2021-05-13T17:24:06.480000000',\n       '2021-05-13T17:52:30.892000000', '2021-05-15T17:42:37.663000000',\n       '2021-05-18T17:52:34.031000000', '2021-05-20T17:30:21.810000000',\n       '2021-05-20T17:42:37.567000000', '2021-05-23T17:52:33.979000000',\n       '2021-05-25T17:42:38.418000000', '2021-05-29T17:24:39.816000000',\n       '2021-06-02T17:52:34.702000000', '2021-06-04T17:42:38.531000000',\n       '2021-06-05T17:30:29.921000000', '2021-06-07T17:52:34.548000000',\n       '2021-06-09T17:42:38.806000000', '2021-06-12T17:52:34.863000000',\n       '2021-06-14T17:24:22.449000000', '2021-06-14T17:42:38.119000000',\n       '2021-06-17T17:52:33.976000000', '2021-06-19T17:42:38.518000000',\n       '2021-06-21T17:30:35.197000000', '2021-06-22T17:52:34.390000000',\n       '2021-06-27T17:52:35.378000000', '2021-06-29T17:42:38.781000000',\n       '2021-06-30T17:24:26.100000000', '2021-07-02T17:52:35.138000000',\n       '2021-07-04T17:42:40.137000000', '2021-07-07T17:30:37.616000000',\n       '2021-07-09T17:42:39.567000000', '2021-07-12T17:52:35.774000000',\n       '2021-07-16T17:24:27.518000000', '2021-07-17T17:52:37.092000000',\n       '2021-07-19T17:42:39.796000000', '2021-07-22T17:52:35.852000000',\n       '2021-07-23T17:30:42.461000000', '2021-07-24T17:42:41.148000000',\n       '2021-07-27T17:52:37.175000000', '2021-07-29T17:42:39.504000000',\n       '2021-08-01T17:24:36.071000000', '2021-08-01T17:52:35.356000000',\n       '2021-08-03T17:42:40.778000000', '2021-08-06T17:52:36.644000000',\n       '2021-08-08T17:30:49.769000000', '2021-08-08T17:42:38.646000000',\n       '2021-08-11T17:52:34.363000000', '2021-08-13T17:42:40.273000000',\n       '2021-08-16T17:52:36.240000000', '2021-08-17T17:24:41.892000000',\n       '2021-08-18T17:42:37.237000000', '2021-08-21T17:52:32.825000000',\n       '2021-08-23T17:42:39.825000000', '2021-08-24T17:30:54.485000000',\n       '2021-08-26T17:52:35.646000000', '2021-08-28T17:42:35.346000000',\n       '2021-08-31T17:52:30.735000000'], dtype='datetime64[ns]')id(time)<U34'HLS.L30.T13TGF.2021124T173013.v...array(['HLS.L30.T13TGF.2021124T173013.v2.0',\n       'HLS.S30.T13TGF.2021125T172901.v2.0',\n       'HLS.S30.T13TGF.2021128T173901.v2.0',\n       'HLS.L30.T13TGF.2021133T172406.v2.0',\n       'HLS.S30.T13TGF.2021133T173859.v2.0',\n       'HLS.S30.T13TGF.2021135T172901.v2.0',\n       'HLS.S30.T13TGF.2021138T173901.v2.0',\n       'HLS.L30.T13TGF.2021140T173021.v2.0',\n       'HLS.S30.T13TGF.2021140T172859.v2.0',\n       'HLS.S30.T13TGF.2021143T173909.v2.0',\n       'HLS.S30.T13TGF.2021145T172901.v2.0',\n       'HLS.L30.T13TGF.2021149T172439.v2.0',\n       'HLS.S30.T13TGF.2021153T173909.v2.0',\n       'HLS.S30.T13TGF.2021155T172901.v2.0',\n       'HLS.L30.T13TGF.2021156T173029.v2.0',\n       'HLS.S30.T13TGF.2021158T173901.v2.0',\n       'HLS.S30.T13TGF.2021160T172859.v2.0',\n       'HLS.S30.T13TGF.2021163T173909.v2.0',\n       'HLS.L30.T13TGF.2021165T172422.v2.0',\n       'HLS.S30.T13TGF.2021165T172901.v2.0',\n...\n       'HLS.S30.T13TGF.2021205T172901.v2.0',\n       'HLS.S30.T13TGF.2021208T173911.v2.0',\n       'HLS.S30.T13TGF.2021210T172859.v2.0',\n       'HLS.L30.T13TGF.2021213T172436.v2.0',\n       'HLS.S30.T13TGF.2021213T173909.v2.0',\n       'HLS.S30.T13TGF.2021215T172901.v2.0',\n       'HLS.S30.T13TGF.2021218T173911.v2.0',\n       'HLS.L30.T13TGF.2021220T173049.v2.0',\n       'HLS.S30.T13TGF.2021220T172859.v2.0',\n       'HLS.S30.T13TGF.2021223T173909.v2.0',\n       'HLS.S30.T13TGF.2021225T172901.v2.0',\n       'HLS.S30.T13TGF.2021228T173911.v2.0',\n       'HLS.L30.T13TGF.2021229T172441.v2.0',\n       'HLS.S30.T13TGF.2021230T172859.v2.0',\n       'HLS.S30.T13TGF.2021233T173859.v2.0',\n       'HLS.S30.T13TGF.2021235T172901.v2.0',\n       'HLS.L30.T13TGF.2021236T173054.v2.0',\n       'HLS.S30.T13TGF.2021238T173911.v2.0',\n       'HLS.S30.T13TGF.2021240T172859.v2.0',\n       'HLS.S30.T13TGF.2021243T173859.v2.0'], dtype='<U34')band(band)<U3'B04' 'B02'array(['B04', 'B02'], dtype='<U3')x(x)float646.97e+05 6.97e+05 ... 8.144e+05array([696990., 697020., 697050., ..., 814350., 814380., 814410.])y(y)float644.604e+06 4.604e+06 ... 4.487e+06array([4603920., 4603890., 4603860., ..., 4486680., 4486650., 4486620.])eo:cloud_cover(time)int6436 52 100 25 22 72 ... 0 30 58 58 0array([ 36,  52, 100,  25,  22,  72, 100,  12,  13,  96,  12, 100,  40,\n         0,   0,   1,  92,   0,   2,   2,  41, 100,  81,   2,  51,  88,\n        99,  48,   0,   1,   0,   2,  72,   0,   0,   0,   0,  42,   0,\n        16,  44,  44,   3,   0,   0,   0,   4,  48,   0,   0,   0,  37,\n         0,  30,  58,  58,   0])end_datetime(time)<U24'2021-05-04T17:30:37.319Z' ... '...array(['2021-05-04T17:30:37.319Z', '2021-05-05T17:42:36.240Z',\n       '2021-05-08T17:52:32.807Z', '2021-05-13T17:24:30.362Z',\n       '2021-05-13T17:52:37.831Z', '2021-05-15T17:42:37.663Z',\n       '2021-05-18T17:52:34.031Z', '2021-05-20T17:30:45.696Z',\n       '2021-05-20T17:42:37.567Z', '2021-05-23T17:52:33.979Z',\n       '2021-05-25T17:42:38.418Z', '2021-05-29T17:24:39.816Z',\n       '2021-06-02T17:52:34.702Z', '2021-06-04T17:42:38.531Z',\n       '2021-06-05T17:30:53.808Z', '2021-06-07T17:52:34.548Z',\n       '2021-06-09T17:42:38.806Z', '2021-06-12T17:52:34.863Z',\n       '2021-06-14T17:24:46.336Z', '2021-06-14T17:42:38.119Z',\n       '2021-06-17T17:52:33.976Z', '2021-06-19T17:42:38.518Z',\n       '2021-06-21T17:30:59.084Z', '2021-06-22T17:52:34.390Z',\n       '2021-06-27T17:52:35.378Z', '2021-06-29T17:42:38.781Z',\n       '2021-06-30T17:24:49.995Z', '2021-07-02T17:52:35.138Z',\n       '2021-07-04T17:42:40.137Z', '2021-07-07T17:31:01.498Z',\n       '2021-07-09T17:42:39.567Z', '2021-07-12T17:52:35.774Z',\n       '2021-07-16T17:24:51.413Z', '2021-07-17T17:52:37.092Z',\n       '2021-07-19T17:42:39.796Z', '2021-07-22T17:52:35.852Z',\n       '2021-07-23T17:31:06.352Z', '2021-07-24T17:42:41.148Z',\n       '2021-07-27T17:52:37.175Z', '2021-07-29T17:42:39.504Z',\n       '2021-08-01T17:24:59.962Z', '2021-08-01T17:52:35.356Z',\n       '2021-08-03T17:42:40.778Z', '2021-08-06T17:52:36.644Z',\n       '2021-08-08T17:31:13.660Z', '2021-08-08T17:42:38.646Z',\n       '2021-08-11T17:52:34.363Z', '2021-08-13T17:42:40.273Z',\n       '2021-08-16T17:52:36.240Z', '2021-08-17T17:25:05.775Z',\n       '2021-08-18T17:42:37.237Z', '2021-08-21T17:52:32.825Z',\n       '2021-08-23T17:42:39.825Z', '2021-08-24T17:31:18.376Z',\n       '2021-08-26T17:52:35.646Z', '2021-08-28T17:42:35.346Z',\n       '2021-08-31T17:52:30.735Z'], dtype='<U24')start_datetime(time)<U24'2021-05-04T17:30:13.428Z' ... '...array(['2021-05-04T17:30:13.428Z', '2021-05-05T17:42:36.240Z',\n       '2021-05-08T17:52:32.807Z', '2021-05-13T17:24:06.480Z',\n       '2021-05-13T17:52:30.892Z', '2021-05-15T17:42:37.663Z',\n       '2021-05-18T17:52:34.031Z', '2021-05-20T17:30:21.810Z',\n       '2021-05-20T17:42:37.567Z', '2021-05-23T17:52:33.979Z',\n       '2021-05-25T17:42:38.418Z', '2021-05-29T17:24:39.816Z',\n       '2021-06-02T17:52:34.702Z', '2021-06-04T17:42:38.531Z',\n       '2021-06-05T17:30:29.921Z', '2021-06-07T17:52:34.548Z',\n       '2021-06-09T17:42:38.806Z', '2021-06-12T17:52:34.863Z',\n       '2021-06-14T17:24:22.449Z', '2021-06-14T17:42:38.119Z',\n       '2021-06-17T17:52:33.976Z', '2021-06-19T17:42:38.518Z',\n       '2021-06-21T17:30:35.197Z', '2021-06-22T17:52:34.390Z',\n       '2021-06-27T17:52:35.378Z', '2021-06-29T17:42:38.781Z',\n       '2021-06-30T17:24:26.100Z', '2021-07-02T17:52:35.138Z',\n       '2021-07-04T17:42:40.137Z', '2021-07-07T17:30:37.616Z',\n       '2021-07-09T17:42:39.567Z', '2021-07-12T17:52:35.774Z',\n       '2021-07-16T17:24:27.518Z', '2021-07-17T17:52:37.092Z',\n       '2021-07-19T17:42:39.796Z', '2021-07-22T17:52:35.852Z',\n       '2021-07-23T17:30:42.461Z', '2021-07-24T17:42:41.148Z',\n       '2021-07-27T17:52:37.175Z', '2021-07-29T17:42:39.504Z',\n       '2021-08-01T17:24:36.071Z', '2021-08-01T17:52:35.356Z',\n       '2021-08-03T17:42:40.778Z', '2021-08-06T17:52:36.644Z',\n       '2021-08-08T17:30:49.769Z', '2021-08-08T17:42:38.646Z',\n       '2021-08-11T17:52:34.363Z', '2021-08-13T17:42:40.273Z',\n       '2021-08-16T17:52:36.240Z', '2021-08-17T17:24:41.892Z',\n       '2021-08-18T17:42:37.237Z', '2021-08-21T17:52:32.825Z',\n       '2021-08-23T17:42:39.825Z', '2021-08-24T17:30:54.485Z',\n       '2021-08-26T17:52:35.646Z', '2021-08-28T17:42:35.346Z',\n       '2021-08-31T17:52:30.735Z'], dtype='<U24')epsg()int6432613array(32613)Attributes: (4)spec :RasterSpec(epsg=32613, bounds=(696990, 4486590, 814440, 4603920), resolutions_xy=(30, 30))crs :epsg:32613transform :| 30.00, 0.00, 696990.00|\n| 0.00,-30.00, 4603920.00|\n| 0.00, 0.00, 1.00|resolution :30\n\n\n\ndata.sel(band='B04').isel(time=[0])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'stackstac-616c3920d64d98e3874f76c6822be2c1' (time: 1,\n                                                                y: 3911, x: 3915)>\ndask.array<getitem, shape=(1, 3911, 3915), dtype=float64, chunksize=(1, 1024, 1024), chunktype=numpy.ndarray>\nCoordinates:\n  * time            (time) datetime64[ns] 2021-05-04T17:30:13.428000\n    id              (time) <U34 'HLS.L30.T13TGF.2021124T173013.v2.0'\n    band            <U3 'B04'\n  * x               (x) float64 6.97e+05 6.97e+05 ... 8.144e+05 8.144e+05\n  * y               (y) float64 4.604e+06 4.604e+06 ... 4.487e+06 4.487e+06\n    eo:cloud_cover  (time) int64 36\n    end_datetime    (time) <U24 '2021-05-04T17:30:37.319Z'\n    start_datetime  (time) <U24 '2021-05-04T17:30:13.428Z'\n    epsg            int64 32613\nAttributes:\n    spec:        RasterSpec(epsg=32613, bounds=(696990, 4486590, 814440, 4603...\n    crs:         epsg:32613\n    transform:   | 30.00, 0.00, 696990.00|\\n| 0.00,-30.00, 4603920.00|\\n| 0.0...\n    resolution:  30xarray.DataArray'stackstac-616c3920d64d98e3874f76c6822be2c1'time: 1y: 3911x: 3915dask.array<chunksize=(1, 1024, 1024), meta=np.ndarray>\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         116.82 MiB \n                         8.00 MiB \n                    \n                    \n                    \n                         Shape \n                         (1, 3911, 3915) \n                         (1, 1024, 1024) \n                    \n                    \n                         Count \n                         2980 Tasks \n                         16 Chunks \n                    \n                    \n                     Type \n                     float64 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  3915\n  3911\n  1\n\n        \n    \nCoordinates: (9)time(time)datetime64[ns]2021-05-04T17:30:13.428000array(['2021-05-04T17:30:13.428000000'], dtype='datetime64[ns]')id(time)<U34'HLS.L30.T13TGF.2021124T173013.v...array(['HLS.L30.T13TGF.2021124T173013.v2.0'], dtype='<U34')band()<U3'B04'array('B04', dtype='<U3')x(x)float646.97e+05 6.97e+05 ... 8.144e+05array([696990., 697020., 697050., ..., 814350., 814380., 814410.])y(y)float644.604e+06 4.604e+06 ... 4.487e+06array([4603920., 4603890., 4603860., ..., 4486680., 4486650., 4486620.])eo:cloud_cover(time)int6436array([36])end_datetime(time)<U24'2021-05-04T17:30:37.319Z'array(['2021-05-04T17:30:37.319Z'], dtype='<U24')start_datetime(time)<U24'2021-05-04T17:30:13.428Z'array(['2021-05-04T17:30:13.428Z'], dtype='<U24')epsg()int6432613array(32613)Attributes: (4)spec :RasterSpec(epsg=32613, bounds=(696990, 4486590, 814440, 4603920), resolutions_xy=(30, 30))crs :epsg:32613transform :| 30.00, 0.00, 696990.00|\n| 0.00,-30.00, 4603920.00|\n| 0.00, 0.00, 1.00|resolution :30"
  },
  {
    "objectID": "notebooks/Search_and_Discovery/Data_Discovery_CMR_API_EDL_Token.html",
    "href": "notebooks/Search_and_Discovery/Data_Discovery_CMR_API_EDL_Token.html",
    "title": "2022 LP DAAC UWG Cloud Workshop",
    "section": "",
    "text": "In this notebook, we will walk through how to search for Earthdata data collections and granules. Along the way we will explore the available search parameters, information return, and specific contrains when using the CMR API. Our object is to identify assets to access that we would downloaded, or perform S3 direct access, within an analysis workflow\nWe will be querying CMR for ECOSTRESS collections/granules to identify assets we would downloaded, or perform S3 direct access, within an analysis workflow\n\n\n\n\n\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\n\nECOSTRESS build 7 is only open to individuals identified as early adopters. As such ECOSTRESS discovery and access is managed by an access control list. If you are not on the access control list, you will not be able to complete the exercise as written below.\n\n\n\n\n\nunderstand what CMR/CMR API is and what CMR/CMR API can be used for\nhow to use the requests package to search data collections and granules\nhow to use an Earthdata Login token to search for data with access control lists\nhow to parse the results of these searches.\n\n\n\n\nCMR is the Common Metadata Repository. It catalogs all data for NASA’s Earth Observing System Data and Information System (EOSDIS). It is the backend of Earthdata Search, the GUI search interface you are probably familiar with. More information about CMR can be found here.\nUnfortunately, the GUI for Earthdata Search is not accessible from a cloud instance - at least not without some work. Earthdata Search is also not immediately reproducible. What I mean by that is if you create a search using the GUI you would have to note the search criteria (date range, search area, collection name, etc), take a screenshot, copy the search url, or save the list of data granules returned by the search, in order to recreate the search. This information would have to be re-entered each time you or someone else wanted to do the search. You could make typos or other mistakes. A cleaner, reproducible solution is to search CMR programmatically using the CMR API.\n\n\n\nAPI stands for Application Programming Interface. It allows applications (software, services, etc) to send information to each other. A helpful analogy is a waiter in a restaurant. The waiter takes your drink or food order that you select from the menu, often translated into short-hand, to the bar or kitchen, and then returns (hopefully) with what you ordered when it is ready.\nThe CMR API accepts search terms such as collection name, keywords, datetime range, and location, queries the CMR database and returns the results.\n\n\n\n\nThe first step is to import python packages. We will use:\n- requests This package does most of the work for us accessing the CMR API using HTTP methods. - pprint to pretty print the results of the search.\nA more in-depth tutorial on requests is here\n\nimport requests\nimport json\nfrom pprint import pprint\n\nTo conduct a search using the CMR API, requests needs the url for the root CMR search endpoint. We’ll assign this url to a python variable as a string.\n\nCMR_OPS = 'https://cmr.earthdata.nasa.gov/search'\n\nCMR allows search by collections, which are datasets, and granules, which are files that contain data. Many of the same search parameters can be used for collections and granules but the type of results returned differ. Search parameters can be found in the API Documentation.\nWhether we search collections or granules is distinguished by adding \"collections\" or \"granules\" to the end of the CMR endpoint URL.\nWe are going to search collections first, so we add \"collections\" to the URL. We are using a python format string in the examples below.\n\nurl = f'{CMR_OPS}/{\"collections\"}'\n\nIn this first example, I want to retrieve a list of ECOSTRESS collections in the Earthdata Cloud. This includes ECOSTRESS collections from build 7 which at the time of this tutorial, is hidden to all except early adopters. Because of this, an extra parameter needs to be passed in each CMR request that indicates you are part of the access list. An Earthdata Login token will be passed to the token parameter, which is generated using your Earthdata Login credentials.\nTwo options are available to generate an Earthdata Login token. 1. Generate a token from the Earthdata Login interface by logging into Earthdata Login and Click Generate Token. 2. Programatically generate an Earthdata Login token. Use the NASA_Earthdata_Login_Token notebook to generate and save a token for use in this notebook.\nWe can read in our token after it has been generated and saved using the NASA_Earthdata_Login_Token notebook. The json file produce can be found here: /home/jovyan/.hidden_dir/edl_token.json. We’ll read to token into a variable named token.\n\nwith open('../../.hidden_dir/edl_token.json') as js:\n    token = json.load(js)['access_token']\n\nWe’ll want to get the content in json (pronounced “jason”) format, so I pass a dictionary to the header keyword argument to say that I want results returned as json.\nThe .get() method is used to send this information to the CMR API. get() calls the HTTP method GET.\n\nresponse = requests.get(url,\n                        params={\n                            'cloud_hosted': 'True',\n                            'has_granules': 'True',\n                        },\n                        headers={\n                            'Accept': 'application/json',\n                        }\n                       )\n\nThe request returns a Response object.\nTo check that our request was successful we can print the response variable we saved the request to.\n\nresponse\n\nA 200 response is what we want. This means that the requests was successful. For more information on HTTP status codes see https://en.wikipedia.org/wiki/List_of_HTTP_status_codes\nA more explict way to check the status code is to use the status_code attribute. Both methods return a HTTP status code.\n\nresponse.status_code\n\nThe response from requests.get returns the results of the search and metadata about those results in the headers.\nMore information about the response object can be found by typing help(response).\nheaders contains useful information in a case-insensitive dictionary. We requested (above) that the information be return in json which means the object return is a dictionary in our Python environment. We’ll iterate through the returned dictionary, looping throught each field (k) and its associated value (v). For more on interating through dictionary object click here.\n\nfor k, v in response.headers.items():\n    print(f'{k}: {v}')\n\nEach item in the dictionary can be accessed in the normal way you access a python dictionary but the keys uniquely case-insensitive. Let’s take a look at the commonly used CMR-Hits key.\n\nresponse.headers['CMR-Hits']\n\nNote that “cmr-hits” works as well!\n\nresponse.headers['cmr-hits']\n\nIn some situations the response to your query can return a very large number of result, some of which may not be relevant. We can add additional query parameters to restrict the information returned. We’re going to restrict the search by the provider parameter.\nYou can modify the code below to explore all Earthdata data products hosted by the various providers. When searching by provider, use Cloud Provider to search for cloud-hosted datasets and On-Premises Provider to search for datasets archived at the DAACs. A partial list of providers is given below.\n\n\n\n\n\n\n\n\n\nDAAC\nShort Name\nCloud Provider\nOn-Premises Provider\n\n\n\n\nNSIDC\nNational Snow and Ice Data Center\nNSIDC_CPRD\nNSIDC_ECS\n\n\nGHRC DAAC\nGlobal Hydrometeorology Resource Center\nGHRC_DAAC\nGHRC_DAAC\n\n\nPO DAAC\nPhysical Oceanography Distributed Active Archive Center\nPOCLOUD\nPODAAC\n\n\nASF\nAlaska Satellite Facility\nASF\nASF\n\n\nORNL DAAC\nOak Ridge National Laboratory\nORNL_CLOUD\nORNL_DAAC\n\n\nLP DAAC\nLand Processes Distributed Active Archive Center\nLPCLOUD\nLPDAAC_ECS\n\n\nGES DISC\nNASA Goddard Earth Sciences (GES) Data and Information Services Center (DISC)\nGES_DISC\nGES_DISC\n\n\nOB DAAC\nNASA’s Ocean Biology Distributed Active Archive Center\n\nOB_DAAC\n\n\nSEDAC\nNASA’s Socioeconomic Data and Applications Center\n\nSEDAC\n\n\n\nWe’ll assign the provider to a variable as a string and insert the variable into the parameter argument in the request. We’ll also assign the term ‘ECOSTRESS’ to a varible so we don’t need to repeatedly add it to the requests parameters.\n\nprovider = 'LPCLOUD'\nproject = 'ECOSTRESS'\n\n\nheaders = {\n    'Authorization': f'Bearer {token}',\n    'Accept': 'application/json',\n}\n\n\nresponse = requests.get(url,\n                        params={\n                            'cloud_hosted': 'True',\n                            'has_granules': 'True',\n                            'provider': provider,\n                            'project': project,\n                        },\n                        headers=headers\n                       )\nresponse\n\n\nresponse.headers['cmr-hits']\n\nSearch results are contained in the content part of the Response object. However, response.content returns information in bytes.\n\nresponse.content\n\nA more convenient way to work with this information is to use json formatted data. I’m using pretty print pprint to print the data in an easy to read way.\nNote - response.json() will format our response in json - ['feed']['entry'] returns all entries that CMR returned in the request (not the same as CMR-Hits) - [0] returns the first entry. Reminder that python starts indexing at 0, not 1!\n\npprint(response.json()['feed']['entry'][0])\n\nThe first response contains a lot more information than we need. We’ll narrow in on a few fields to get a feel for what we have. We’ll print the name of the dataset (dataset_id) and the concept id (id). We can build this variable and print statement like we did above with the url variable.\n\ncollections = response.json()['feed']['entry']\n\n\nfor collection in collections:\n    print(f'{collection[\"archive_center\"]} | {collection[\"dataset_id\"]} | {collection[\"id\"]}')\n\nIn some situations we may be expecting a certain number of results. Note here that we only have 10 datasets are printed. We know from CMR-Hits that there are more than 10 datasets. This is because CMR restricts the number of results returned by each query. The default is 10 but it can be set to a maximum of 2000. We’ll set the page_size parameter to 25 so we return all results in a single query.\n\nresponse = requests.get(url,\n                        params={\n                            'cloud_hosted': 'True',\n                            'has_granules': 'True',\n                            'provider': provider,\n                            'project': project,\n                            'page_size': 25\n                        },\n                        headers=headers\n                       )\nresponse\n\n\nresponse.headers['cmr-hits']\n\nNow, when we can re-run our for loop for the collections we now have all of the available collections listed.\n\ncollections = response.json()['feed']['entry']\nfor collection in collections:\n    print(f'{collection[\"archive_center\"]} | {collection[\"dataset_id\"]} | {collection[\"id\"]}')\n\n\n\n\nIn NASA speak, Granules are files or groups of files. In this example, we will search for ECO2LSTE version 1 for a specified region of interest and datetime range.\nWe need to change the resource url to look for granules instead of collections\n\nurl = f'{CMR_OPS}/{\"granules\"}'\n\nWe will search by concept_id, temporal, and bounding_box. Details about these search parameters can be found in the CMR API Documentation.\nThe formatting of the values for each parameter is quite specific.\nTemporal parameters are in ISO 8061 format yyyy-MM-ddTHH:mm:ssZ.\nBounding box coordinates are lower left longitude, lower left latitude, upper right longitude, upper right latitude.\n\ncollection_id = 'C2076090826-LPCLOUD'\ndate_range = '2022-04-01T00:00:00Z,2022-04-30T23:59:59Z'\nbbox = '-120.45264628,34.51050622,-120.40432448,34.53239876'\n\n\nresponse = requests.get(url, \n                        params={\n                            'concept_id': collection_id,\n                            'temporal': date_range,\n                            'bounding_box': bbox,\n                            'token': token,\n                            'page_size': 200\n                            },\n                        headers=headers\n                       )\nprint(response.status_code)\n\n\nprint(response.headers['CMR-Hits'])\n\n\ngranules = response.json()['feed']['entry']\nfor granule in granules:\n    print(f'{granule[\"data_center\"]} | {granule[\"dataset_id\"]} | {granule[\"id\"]}')\n\n\npprint(granules[0])\n\n\n\n\n\nhttps_urls = [l['href'] for l in granules[0]['links'] if 'https' in l['href'] and '.tif' in l['href']]\nhttps_urls\n\n\ns3_urls = [l['href'] for l in granules[0]['links'] if 's3' in l['href'] and '.tif' in l['href']]\ns3_urls"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "The ECOSTRESS Cloud Workshop take place on April 12-13, from 2pm-5:30pm PDT.\nWebEx links will be shared directly with this group via a (calendar) meeting invite.\nNote, hands-on exercises will be executed from a Jupyter Lab instance in 2i2c. Please pass along your Github Username to get access."
  },
  {
    "objectID": "schedule.html#workshop-schedule",
    "href": "schedule.html#workshop-schedule",
    "title": "Schedule",
    "section": "Workshop Schedule",
    "text": "Workshop Schedule\n\nDay 1 - April 12, 2022\n\n\n\nTime, PST (UTC-7)\nEvent\nLeads/Instructors\n\n\n\n\n2:00 pm\nWelcome / Workshop Expectations\nAaron Friesz (LP DAAC) / Christine Lee (JPL)\n\n\n2:10 pm\nECOSTRESS Build 7 collection Overview\nChristine Lee & Gregory Halverson (JPL)\n\n\n2:25 pm\nEarthdata Cloud Overview\nAaron Friesz (LP DAAC)\n\n\n2:55 pm\nBreak/QA\n\n\n\n3:00 pm\nEarthdata Search Client (GUI)\nAaron Friesz (LP DAAC)\n\n\n3:30 pm\nBreak\n\n\n\n4:00 pm\nGetting set up / Intro to Pangeo\nAaron Friesz (LP DAAC)\n\n\n4:30 pm\nIntro to xarray & hvplot\nAaron Friesz (LP DAAC)\n\n\n5:15 pm\nBreak/QA\n\n\n\n\n\nClosing Day 1\n\nThank you!\nJupyterHub: close out.\n\nclose out your JupyterHub instance if you are finished for the day, following these instructions.\n\nContinued work.\n\nYou’re welcome to continue working beyond the workshop daily scheduled session, using JupyterHub.\n\nAgenda for tomorrow: what’s coming next.\n\n\n\n\nDay 2 - April 13, 2022\n\n\n\nTime, PST (UTC-7)\nEvent\nLeads/Instructors\n\n\n\n\n2:00 pm\nWelcome Back / Earthdata Authentication: Set up netrc file & Generate EDL Tokens\nAaron Friesz (LP DAAC)\n\n\n2:10 pm\nEarthdata Cloud: Search and Discovery - CMR API\nAaron Friesz (LP DAAC)\n\n\n2:45 pm\nBreak/QA\n\n\n\n2:50 pm\nEarthdata Cloud: Search and Discovery - Data Access\nAaron Friesz (LP DAAC)\n\n\n3:30 pm\nBreak\n\n\n\n4:00 pm\nComparing TNC Tree Data to ECOSTRESS Use Case\nGregory Halverson (JPL)\n\n\n5:00 pm\nBreak/QA\n\n\n\n\n\nClosing Day 2\n\nThank you!\nYou will continue to have access to the 2i2c JupyterHub in AWS for two weeks following the ECOSTRESS Cloud Workshop. You may use that time to continue work and all learn more about migrating data accass routines and science workflows to the Cloud. This cloud compute environment is supported by the NASA Openscapes project."
  },
  {
    "objectID": "schedule.html#getting-help-during-the-workshop",
    "href": "schedule.html#getting-help-during-the-workshop",
    "title": "Schedule",
    "section": "Getting help during the Workshop",
    "text": "Getting help during the Workshop\nWe will use the ECOSTRESS Slack Workspace as our main channels for help. Please use Slack to post questions."
  },
  {
    "objectID": "further-resources.html",
    "href": "further-resources.html",
    "title": "Additional resources",
    "section": "",
    "text": "One stop for PO.DAAC Cloud Information: Cloud Data page with About, Cloud Datasets, Access Data, FAQs, Resources, and Migration information\nAsk questions or find resources: PO.DAAC in the CLOUD Forum\nCloud user migration overview, guidance, and resources: PO.DAAC Webinar\nSearch and get access links: Earthdata Search Client and guide\nSearch and get access links: PO.DAAC Cloud Earthdata Search Portal\nBrowse cloud data in web-based browser: CMR Virtual Browse and guiding video\nScripted data search end-point: Earthdata Common Metadata Repository (CMR) API\nEnable data download or access: Obtain Earthdata Login Account\nDownload data regularly: PO.DAAC Data Subscriber Access video and PO.DAAC Data Subscriber instructions\nBulk Download guide\nOPeNDAP in the cloud\nPO.DAAC scripts and notebooks: PO.DAAC Github\nHow to get started in the AWS cloud: Earthdata Cloud Primer documents\n2021 NASA Cloud Hackathon - November 2021, co-hosted by PODAAC, NSIDC DAAC, and LPDAAC. Additional support is provided by ASDC, GESDISC, IMPACT, and Openscapes.\nNASA Earthdata: How to Cloud\nSetting up Jupyter Notebooks in a user EC2 instance in AWS - helpful blog post for setting up jupyter notebooks in an EC2 instance in AWS. (Builds on the Cloud Primer tutorials, which are missing that next step)"
  },
  {
    "objectID": "further-resources.html#additional-tutorials",
    "href": "further-resources.html#additional-tutorials",
    "title": "Additional resources",
    "section": "Additional tutorials",
    "text": "Additional tutorials\n\nData_Access__Direct_S3_Access__PODAAC_ECCO_SSH using CMR-STAC API to retrieve S3 links\nDirect access to ECCO data in S3 (from us-west-2) - Direct S3 access example with netCDF data\nDirect_S3_Access__gdalvrt\nDirect_S3_Access__rioxarray_clipping\nCalculate black-sky, white-sky, and actual albedo (MCD43A) from MCD43A1 BRDF Parameters using R\nXarray Zonal Statistics"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "",
    "text": "Welcome to the 2022 ECOSTRESS Cloud Workshop hosted by NASA’s Land Processes Distributed Activate Archive (LP DAAC) with support from NASA Openscapes.\nThe workshop will take place virtually daily on April 12 and 13, 2022 from 2pm-5:30pm PST (UTC-7)."
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "About",
    "text": "About\n\nWorkshop Goal\nThe goal of the workshop is expose ECOSTRESS data users to ECOSTRESS version 2 (v2) data products in the cloud. Learning objectives focus on how to find and access ECOSTRESS v2 data from Earthdata Cloud either by downloading or accessing the data on the cloud. The LP DAAC is the NASA archive for ECOSTRESS data products. ECOSTRESS v2 data products will hosted in the NASA Earthdata Cloud, hosted in AWS.\n\n\nWorkshop Description\nThe goal of the workshop is expose ECOSTRESS data users to ECOSTRESS version 2 data products in the cloud. Learning objectives focus on how to finda and access ECOSTRESS version 2 data from Earthdata Cloud either by downloading or accessing the data on the cloud. The LP DAAC is the NASA archive for ECOSTRESS data products. ECOSTRESS version 2 data products will hosted in the NASA Earthdata Cloud, hosted in AWS.\nThe workshop will demonstrate how to find, access, and download ECOSTRESS v2 data from the Earthdata Cloud. Participants will learn how to search for and download data from NASA’s Earthdata Search Client, a graphical user interface (GUI) for search, discovery, and download application for also EOSDIS data assets. Participants will also learn how to perform in-could data search, access, and processing routines where no data download is required, and data analysis can take place next to the data in the cloud.\n\n\nWorkshop Outcomes\nAt the end of the two days, participants should be able to find and access ECOSTRESS v2 data in the NASA Earthdata Cloud (hosted in AWS). Workshop materials will be available for future reference following the completion of the workshop/ECOSTRESS Science Team meeting\n\nNOTE: ECOSTRESS v2 data will only be available to approved individuals. Please work with Christine Lee (christine.m.lee@jpl.nasa.gov) to have your name added to the allowlist."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "2022 ECOSTRESS Cloud Workshop",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n2022 ECOSTRESS Cloud Workshop is hosted by NASA’s LP DAAC with support from the NASA Openscapes Project, with cloud computing infrastructure by 2i2c."
  }
]